{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()  # take environment variables from .env."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "\n",
    "class Question(BaseModel):\n",
    "    \"\"\" A question about a domain of study.\"\"\"\n",
    "\n",
    "    topic_description: str = Field(\n",
    "        description=\"A sentence describing the sub-topic to which the question belongs. That means this sentence should specify in a granular level what specific sub-topic the question belongs to. It should be abstract in a way that other questions could be put in this description too. Use between 5 and 10 words.\"\n",
    "    )\n",
    "    level: float = Field(\n",
    "        description=\"The difficulty level of the question. It should be only one of the following options: 'easy', 'medium', 'hard'.\"\n",
    "    )\n",
    "    question: str = Field(\n",
    "        description=\"The actual question text. It should be a question of type TRUE or FALSE. It means that the questions should be an assertion that could be answered with TRUE or FALSE.\"\n",
    "    )\n",
    "    answer_correct: str = Field(\n",
    "        description=\"The correct answer to the question. It should be only one of the following options: TRUE or FALSE\"\n",
    "    )\n",
    "    explanation: str = Field(\n",
    "        description=\"An explanation or solution to the question.\"\n",
    "    )\n",
    "\n",
    "class QuestionList(BaseModel):\n",
    "    \"\"\"A list of Question class.\"\"\"\n",
    "\n",
    "    question_list: list[Question] = Field(\n",
    "        description=\"list of Question\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspaces/llm-quiz-creator-streamlitapp-trainer/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import asyncio\n",
    "import base64\n",
    "import json\n",
    "import logging\n",
    "import os\n",
    "import uuid\n",
    "import warnings\n",
    "from io import BytesIO\n",
    "from operator import itemgetter\n",
    "from typing import (\n",
    "    Any,\n",
    "    AsyncIterator,\n",
    "    Callable,\n",
    "    Dict,\n",
    "    Iterator,\n",
    "    List,\n",
    "    Mapping,\n",
    "    Optional,\n",
    "    Sequence,\n",
    "    Tuple,\n",
    "    Type,\n",
    "    Union,\n",
    "    cast,\n",
    ")\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "import google.api_core\n",
    "\n",
    "# TODO: remove ignore once the google package is published with types\n",
    "import proto  # type: ignore[import]\n",
    "import requests\n",
    "from google.ai.generativelanguage_v1beta.types import (\n",
    "    Candidate,\n",
    "    Content,\n",
    "    FunctionCall,\n",
    "    FunctionResponse,\n",
    "    GenerateContentRequest,\n",
    "    GenerateContentResponse,\n",
    "    GenerationConfig,\n",
    "    Part,\n",
    "    SafetySetting,\n",
    "    ToolConfig,\n",
    ")\n",
    "from google.generativeai.types import Tool as GoogleTool  # type: ignore[import]\n",
    "from google.generativeai.types.content_types import (  # type: ignore[import]\n",
    "    FunctionDeclarationType,\n",
    "    ToolDict,\n",
    ")\n",
    "from langchain_core.callbacks.manager import (\n",
    "    AsyncCallbackManagerForLLMRun,\n",
    "    CallbackManagerForLLMRun,\n",
    ")\n",
    "from langchain_core.language_models import LanguageModelInput\n",
    "from langchain_core.language_models.chat_models import BaseChatModel, LangSmithParams\n",
    "from langchain_core.messages import (\n",
    "    AIMessage,\n",
    "    AIMessageChunk,\n",
    "    BaseMessage,\n",
    "    FunctionMessage,\n",
    "    HumanMessage,\n",
    "    SystemMessage,\n",
    "    ToolMessage,\n",
    ")\n",
    "from langchain_core.messages.ai import UsageMetadata\n",
    "from langchain_core.messages.tool import invalid_tool_call, tool_call, tool_call_chunk\n",
    "from langchain_core.output_parsers.base import OutputParserLike\n",
    "from langchain_core.output_parsers.openai_tools import (\n",
    "    JsonOutputToolsParser,\n",
    "    PydanticToolsParser,\n",
    "    parse_tool_calls,\n",
    ")\n",
    "from langchain_core.outputs import ChatGeneration, ChatGenerationChunk, ChatResult\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field, SecretStr, root_validator\n",
    "from langchain_core.runnables import Runnable, RunnablePassthrough\n",
    "from langchain_core.utils import get_from_dict_or_env\n",
    "from tenacity import (\n",
    "    before_sleep_log,\n",
    "    retry,\n",
    "    retry_if_exception_type,\n",
    "    stop_after_attempt,\n",
    "    wait_exponential,\n",
    ")\n",
    "\n",
    "from langchain_google_genai._common import (\n",
    "    GoogleGenerativeAIError,\n",
    "    SafetySettingDict,\n",
    "    get_client_info,\n",
    ")\n",
    "from langchain_google_genai._function_utils import (\n",
    "    _tool_choice_to_tool_config,\n",
    "    _ToolChoiceType,\n",
    "    _ToolConfigDict,\n",
    "    convert_to_genai_function_declarations,\n",
    "    # tool_to_dict,\n",
    ")\n",
    "from langchain_google_genai._image_utils import ImageBytesLoader\n",
    "from langchain_google_genai.llms import _BaseGoogleGenerativeAI\n",
    "\n",
    "from langchain_google_genai import _genai_extension as genaix\n",
    "# from . import _genai_extension as genaix\n",
    "\n",
    "IMAGE_TYPES: Tuple = ()\n",
    "try:\n",
    "    import PIL\n",
    "    from PIL.Image import Image\n",
    "\n",
    "    IMAGE_TYPES = IMAGE_TYPES + (Image,)\n",
    "except ImportError:\n",
    "    PIL = None  # type: ignore\n",
    "    Image = None  # type: ignore\n",
    "\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tool_to_dict(tool: gapic.Tool) -> _ToolDict:\n",
    "\n",
    "    print(\"---------------------------\")\n",
    "    print(\"tool\")\n",
    "    print(tool)\n",
    "    print(type(tool))\n",
    "\n",
    "    print(\"---------------------------\")\n",
    "\n",
    "    t = type(tool).to_dict(tool)\n",
    "    print(\"type(tool).to_dict(tool)\")\n",
    "    print(t)\n",
    "    print(\"---------------------------\")\n",
    "    \n",
    "    print(\"type(t)\")\n",
    "    print(type(t))\n",
    "    print(\"---------------------------\")\n",
    "\n",
    "    def _traverse_values(raw: Any) -> Any:\n",
    "        if isinstance(raw, list):\n",
    "            return [_traverse_values(v) for v in raw]\n",
    "        if isinstance(raw, dict):\n",
    "            return {k: _traverse_values(v) for k, v in raw.items()}\n",
    "        if isinstance(raw, proto.Message):\n",
    "            return _traverse_values(type(raw).to_dict(raw))\n",
    "        return raw\n",
    "\n",
    "    return _traverse_values(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def _parse_response_candidate(\n",
    "    response_candidate: Candidate, streaming: bool = False\n",
    ") -> AIMessage:\n",
    "    content: Union[None, str, List[str]] = None\n",
    "    additional_kwargs = {}\n",
    "    tool_calls = []\n",
    "    invalid_tool_calls = []\n",
    "    tool_call_chunks = []\n",
    "\n",
    "    for part in response_candidate.content.parts:\n",
    "        try:\n",
    "            text: Optional[str] = part.text\n",
    "        except AttributeError:\n",
    "            text = None\n",
    "\n",
    "        if text is not None:\n",
    "            if not content:\n",
    "                content = text\n",
    "            elif isinstance(content, str) and text:\n",
    "                content = [content, text]\n",
    "            elif isinstance(content, list) and text:\n",
    "                content.append(text)\n",
    "            elif text:\n",
    "                raise Exception(\"Unexpected content type\")\n",
    "\n",
    "        if part.function_call:\n",
    "            function_call = {\"name\": part.function_call.name}\n",
    "            # dump to match other function calling llm for now\n",
    "            function_call_args_dict = proto.Message.to_dict(part.function_call)[\"args\"]\n",
    "            function_call[\"arguments\"] = json.dumps(\n",
    "                {k: function_call_args_dict[k] for k in function_call_args_dict}\n",
    "            )\n",
    "            additional_kwargs[\"function_call\"] = function_call\n",
    "\n",
    "            if streaming:\n",
    "                tool_call_chunks.append(\n",
    "                    tool_call_chunk(\n",
    "                        name=function_call.get(\"name\"),\n",
    "                        args=function_call.get(\"arguments\"),\n",
    "                        id=function_call.get(\"id\", str(uuid.uuid4())),\n",
    "                        index=function_call.get(\"index\"),  # type: ignore\n",
    "                    )\n",
    "                )\n",
    "            else:\n",
    "                try:\n",
    "                    tool_call_dict = parse_tool_calls(\n",
    "                        [{\"function\": function_call}],\n",
    "                        return_id=False,\n",
    "                    )[0]\n",
    "                except Exception as e:\n",
    "                    invalid_tool_calls.append(\n",
    "                        invalid_tool_call(\n",
    "                            name=function_call.get(\"name\"),\n",
    "                            args=function_call.get(\"arguments\"),\n",
    "                            id=function_call.get(\"id\", str(uuid.uuid4())),\n",
    "                            error=str(e),\n",
    "                        )\n",
    "                    )\n",
    "                else:\n",
    "                    tool_calls.append(\n",
    "                        tool_call(\n",
    "                            name=tool_call_dict[\"name\"],\n",
    "                            args=tool_call_dict[\"args\"],\n",
    "                            id=tool_call_dict.get(\"id\", str(uuid.uuid4())),\n",
    "                        )\n",
    "                    )\n",
    "    if content is None:\n",
    "        content = \"\"\n",
    "\n",
    "    if streaming:\n",
    "        return AIMessageChunk(\n",
    "            content=cast(Union[str, List[Union[str, Dict[Any, Any]]]], content),\n",
    "            additional_kwargs=additional_kwargs,\n",
    "            tool_call_chunks=tool_call_chunks,\n",
    "        )\n",
    "\n",
    "    return AIMessage(\n",
    "        content=cast(Union[str, List[Union[str, Dict[Any, Any]]]], content),\n",
    "        additional_kwargs=additional_kwargs,\n",
    "        tool_calls=tool_calls,\n",
    "        invalid_tool_calls=invalid_tool_calls,\n",
    "    )\n",
    "\n",
    "\n",
    "def _response_to_result(\n",
    "    response: GenerateContentResponse,\n",
    "    stream: bool = False,\n",
    ") -> ChatResult:\n",
    "    \"\"\"Converts a PaLM API response into a LangChain ChatResult.\"\"\"\n",
    "    llm_output = {\"prompt_feedback\": proto.Message.to_dict(response.prompt_feedback)}\n",
    "\n",
    "    # Get usage metadata\n",
    "    try:\n",
    "        input_tokens = response.usage_metadata.prompt_token_count\n",
    "        output_tokens = response.usage_metadata.candidates_token_count\n",
    "        total_tokens = response.usage_metadata.total_token_count\n",
    "        if input_tokens + output_tokens + total_tokens > 0:\n",
    "            lc_usage = UsageMetadata(\n",
    "                input_tokens=input_tokens,\n",
    "                output_tokens=output_tokens,\n",
    "                total_tokens=total_tokens,\n",
    "            )\n",
    "        else:\n",
    "            lc_usage = None\n",
    "    except AttributeError:\n",
    "        lc_usage = None\n",
    "\n",
    "    generations: List[ChatGeneration] = []\n",
    "\n",
    "    for candidate in response.candidates:\n",
    "        generation_info = {}\n",
    "        if candidate.finish_reason:\n",
    "            generation_info[\"finish_reason\"] = candidate.finish_reason.name\n",
    "        generation_info[\"safety_ratings\"] = [\n",
    "            proto.Message.to_dict(safety_rating, use_integers_for_enums=False)\n",
    "            for safety_rating in candidate.safety_ratings\n",
    "        ]\n",
    "        message = _parse_response_candidate(candidate, streaming=stream)\n",
    "        message.usage_metadata = lc_usage\n",
    "        generations.append(\n",
    "            (ChatGenerationChunk if stream else ChatGeneration)(\n",
    "                message=message,\n",
    "                generation_info=generation_info,\n",
    "            )\n",
    "        )\n",
    "    if not response.candidates:\n",
    "        # Likely a \"prompt feedback\" violation (e.g., toxic input)\n",
    "        # Raising an error would be different than how OpenAI handles it,\n",
    "        # so we'll just log a warning and continue with an empty message.\n",
    "        logger.warning(\n",
    "            \"Gemini produced an empty response. Continuing with empty message\\n\"\n",
    "            f\"Feedback: {response.prompt_feedback}\"\n",
    "        )\n",
    "        generations = [\n",
    "            (ChatGenerationChunk if stream else ChatGeneration)(\n",
    "                message=(AIMessageChunk if stream else AIMessage)(content=\"\"),\n",
    "                generation_info={},\n",
    "            )\n",
    "        ]\n",
    "    return ChatResult(generations=generations, llm_output=llm_output)\n",
    "\n",
    "\n",
    "class ChatGoogleGenerativeAIError(GoogleGenerativeAIError):\n",
    "    \"\"\"\n",
    "    Custom exception class for errors associated with the `Google GenAI` API.\n",
    "\n",
    "    This exception is raised when there are specific issues related to the\n",
    "    Google genai API usage in the ChatGoogleGenerativeAI class, such as unsupported\n",
    "    message types or roles.\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "def _create_retry_decorator() -> Callable[[Any], Any]:\n",
    "    \"\"\"\n",
    "    Creates and returns a preconfigured tenacity retry decorator.\n",
    "\n",
    "    The retry decorator is configured to handle specific Google API exceptions\n",
    "    such as ResourceExhausted and ServiceUnavailable. It uses an exponential\n",
    "    backoff strategy for retries.\n",
    "\n",
    "    Returns:\n",
    "        Callable[[Any], Any]: A retry decorator configured for handling specific\n",
    "        Google API exceptions.\n",
    "    \"\"\"\n",
    "    multiplier = 2\n",
    "    min_seconds = 1\n",
    "    max_seconds = 60\n",
    "    max_retries = 2\n",
    "\n",
    "    return retry(\n",
    "        reraise=True,\n",
    "        stop=stop_after_attempt(max_retries),\n",
    "        wait=wait_exponential(multiplier=multiplier, min=min_seconds, max=max_seconds),\n",
    "        retry=(\n",
    "            retry_if_exception_type(google.api_core.exceptions.ResourceExhausted)\n",
    "            | retry_if_exception_type(google.api_core.exceptions.ServiceUnavailable)\n",
    "            | retry_if_exception_type(google.api_core.exceptions.GoogleAPIError)\n",
    "        ),\n",
    "        before_sleep=before_sleep_log(logger, logging.WARNING),\n",
    "    )\n",
    "\n",
    "\n",
    "def _chat_with_retry(generation_method: Callable, **kwargs: Any) -> Any:\n",
    "    \"\"\"\n",
    "    Executes a chat generation method with retry logic using tenacity.\n",
    "\n",
    "    This function is a wrapper that applies a retry mechanism to a provided\n",
    "    chat generation function. It is useful for handling intermittent issues\n",
    "    like network errors or temporary service unavailability.\n",
    "\n",
    "    Args:\n",
    "        generation_method (Callable): The chat generation method to be executed.\n",
    "        **kwargs (Any): Additional keyword arguments to pass to the generation method.\n",
    "\n",
    "    Returns:\n",
    "        Any: The result from the chat generation method.\n",
    "    \"\"\"\n",
    "    retry_decorator = _create_retry_decorator()\n",
    "\n",
    "    @retry_decorator\n",
    "    def _chat_with_retry(**kwargs: Any) -> Any:\n",
    "        try:\n",
    "            return generation_method(**kwargs)\n",
    "        # Do not retry for these errors.\n",
    "        except google.api_core.exceptions.FailedPrecondition as exc:\n",
    "            if \"location is not supported\" in exc.message:\n",
    "                error_msg = (\n",
    "                    \"Your location is not supported by google-generativeai \"\n",
    "                    \"at the moment. Try to use ChatVertexAI LLM from \"\n",
    "                    \"langchain_google_vertexai.\"\n",
    "                )\n",
    "                raise ValueError(error_msg)\n",
    "\n",
    "        except google.api_core.exceptions.InvalidArgument as e:\n",
    "            raise ChatGoogleGenerativeAIError(\n",
    "                f\"Invalid argument provided to Gemini: {e}\"\n",
    "            ) from e\n",
    "        except Exception as e:\n",
    "            raise e\n",
    "\n",
    "    return _chat_with_retry(**kwargs)\n",
    "\n",
    "\n",
    "def _convert_to_parts(\n",
    "    raw_content: Union[str, Sequence[Union[str, dict]]],\n",
    ") -> List[Part]:\n",
    "    \"\"\"Converts a list of LangChain messages into a google parts.\"\"\"\n",
    "    parts = []\n",
    "    content = [raw_content] if isinstance(raw_content, str) else raw_content\n",
    "    image_loader = ImageBytesLoader()\n",
    "    for part in content:\n",
    "        if isinstance(part, str):\n",
    "            parts.append(Part(text=part))\n",
    "        elif isinstance(part, Mapping):\n",
    "            # OpenAI Format\n",
    "            if _is_openai_parts_format(part):\n",
    "                if part[\"type\"] == \"text\":\n",
    "                    parts.append(Part(text=part[\"text\"]))\n",
    "                elif part[\"type\"] == \"image_url\":\n",
    "                    img_url = part[\"image_url\"]\n",
    "                    if isinstance(img_url, dict):\n",
    "                        if \"url\" not in img_url:\n",
    "                            raise ValueError(\n",
    "                                f\"Unrecognized message image format: {img_url}\"\n",
    "                            )\n",
    "                        img_url = img_url[\"url\"]\n",
    "                    parts.append(image_loader.load_part(img_url))\n",
    "                else:\n",
    "                    raise ValueError(f\"Unrecognized message part type: {part['type']}\")\n",
    "            else:\n",
    "                # Yolo\n",
    "                logger.warning(\n",
    "                    \"Unrecognized message part format. Assuming it's a text part.\"\n",
    "                )\n",
    "                parts.append(Part(text=str(part)))\n",
    "        else:\n",
    "            # TODO: Maybe some of Google's native stuff\n",
    "            # would hit this branch.\n",
    "            raise ChatGoogleGenerativeAIError(\n",
    "                \"Gemini only supports text and inline_data parts.\"\n",
    "            )\n",
    "    return parts\n",
    "\n",
    "\n",
    "def _parse_chat_history(\n",
    "    input_messages: Sequence[BaseMessage], convert_system_message_to_human: bool = False\n",
    ") -> Tuple[Optional[Content], List[Content]]:\n",
    "    messages: List[Content] = []\n",
    "\n",
    "    if convert_system_message_to_human:\n",
    "        warnings.warn(\"Convert_system_message_to_human will be deprecated!\")\n",
    "\n",
    "    system_instruction: Optional[Content] = None\n",
    "    for i, message in enumerate(input_messages):\n",
    "        if i == 0 and isinstance(message, SystemMessage):\n",
    "            system_instruction = Content(parts=_convert_to_parts(message.content))\n",
    "            continue\n",
    "        elif isinstance(message, AIMessage):\n",
    "            role = \"model\"\n",
    "            raw_function_call = message.additional_kwargs.get(\"function_call\")\n",
    "            if raw_function_call:\n",
    "                function_call = FunctionCall(\n",
    "                    {\n",
    "                        \"name\": raw_function_call[\"name\"],\n",
    "                        \"args\": json.loads(raw_function_call[\"arguments\"]),\n",
    "                    }\n",
    "                )\n",
    "                parts = [Part(function_call=function_call)]\n",
    "            else:\n",
    "                parts = _convert_to_parts(message.content)\n",
    "        elif isinstance(message, HumanMessage):\n",
    "            role = \"user\"\n",
    "            parts = _convert_to_parts(message.content)\n",
    "            if i == 1 and convert_system_message_to_human and system_instruction:\n",
    "                parts = [p for p in system_instruction.parts] + parts\n",
    "                system_instruction = None\n",
    "        elif isinstance(message, FunctionMessage):\n",
    "            role = \"user\"\n",
    "            response: Any\n",
    "            if not isinstance(message.content, str):\n",
    "                response = message.content\n",
    "            else:\n",
    "                try:\n",
    "                    response = json.loads(message.content)\n",
    "                except json.JSONDecodeError:\n",
    "                    response = message.content  # leave as str representation\n",
    "            parts = [\n",
    "                Part(\n",
    "                    function_response=FunctionResponse(\n",
    "                        name=message.name,\n",
    "                        response=(\n",
    "                            {\"output\": response}\n",
    "                            if not isinstance(response, dict)\n",
    "                            else response\n",
    "                        ),\n",
    "                    )\n",
    "                )\n",
    "            ]\n",
    "        elif isinstance(message, ToolMessage):\n",
    "            role = \"user\"\n",
    "            prev_message: Optional[BaseMessage] = (\n",
    "                input_messages[i - 1] if i > 0 else None\n",
    "            )\n",
    "            if (\n",
    "                prev_message\n",
    "                and isinstance(prev_message, AIMessage)\n",
    "                and prev_message.tool_calls\n",
    "            ):\n",
    "                # message.name can be null for ToolMessage\n",
    "                name: str = prev_message.tool_calls[0][\"name\"]\n",
    "            else:\n",
    "                name = message.name  # type: ignore\n",
    "            tool_response: Any\n",
    "            if not isinstance(message.content, str):\n",
    "                tool_response = message.content\n",
    "            else:\n",
    "                try:\n",
    "                    tool_response = json.loads(message.content)\n",
    "                except json.JSONDecodeError:\n",
    "                    tool_response = message.content  # leave as str representation\n",
    "            parts = [\n",
    "                Part(\n",
    "                    function_response=FunctionResponse(\n",
    "                        name=name,\n",
    "                        response=(\n",
    "                            {\"output\": tool_response}\n",
    "                            if not isinstance(tool_response, dict)\n",
    "                            else tool_response\n",
    "                        ),\n",
    "                    )\n",
    "                )\n",
    "            ]\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                f\"Unexpected message with type {type(message)} at the position {i}.\"\n",
    "            )\n",
    "\n",
    "        messages.append(Content(role=role, parts=parts))\n",
    "    return system_instruction, messages\n",
    "\n",
    "\n",
    "def _is_event_loop_running() -> bool:\n",
    "    try:\n",
    "        asyncio.get_running_loop()\n",
    "        return True\n",
    "    except RuntimeError:\n",
    "        return False\n",
    "\n",
    "class ChatGoogleGenerativeAI(_BaseGoogleGenerativeAI, BaseChatModel):\n",
    "    \"\"\"`Google AI` chat models integration.\n",
    "\n",
    "    Instantiation:\n",
    "        To use, you must have either:\n",
    "\n",
    "            1. The ``GOOGLE_API_KEY``` environment variable set with your API key, or\n",
    "            2. Pass your API key using the google_api_key kwarg to the ChatGoogle\n",
    "               constructor.\n",
    "\n",
    "        .. code-block:: python\n",
    "\n",
    "            from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "            llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-pro\")\n",
    "            llm.invoke(\"Write me a ballad about LangChain\")\n",
    "\n",
    "    Invoke:\n",
    "        .. code-block:: python\n",
    "\n",
    "            messages = [\n",
    "                (\"system\", \"Translate the user sentence to French.\"),\n",
    "                (\"human\", \"I love programming.\"),\n",
    "            ]\n",
    "            llm.invoke(messages)\n",
    "\n",
    "        .. code-block:: python\n",
    "\n",
    "            AIMessage(\n",
    "                content=\"J'adore programmer. \\\\n\",\n",
    "                response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': [{'category': 'HARM_CATEGORY_SEXUALLY_EXPLICIT', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_HATE_SPEECH', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_HARASSMENT', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_DANGEROUS_CONTENT', 'probability': 'NEGLIGIBLE', 'blocked': False}]},\n",
    "                id='run-56cecc34-2e54-4b52-a974-337e47008ad2-0',\n",
    "                usage_metadata={'input_tokens': 18, 'output_tokens': 5, 'total_tokens': 23}\n",
    "            )\n",
    "\n",
    "    Stream:\n",
    "        .. code-block:: python\n",
    "\n",
    "            for chunk in llm.stream(messages):\n",
    "                print(chunk)\n",
    "\n",
    "        .. code-block:: python\n",
    "\n",
    "            AIMessageChunk(content='J', response_metadata={'finish_reason': 'STOP', 'safety_ratings': []}, id='run-e905f4f4-58cb-4a10-a960-448a2bb649e3', usage_metadata={'input_tokens': 18, 'output_tokens': 1, 'total_tokens': 19})\n",
    "            AIMessageChunk(content=\"'adore programmer. \\n\", response_metadata={'finish_reason': 'STOP', 'safety_ratings': [{'category': 'HARM_CATEGORY_SEXUALLY_EXPLICIT', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_HATE_SPEECH', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_HARASSMENT', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_DANGEROUS_CONTENT', 'probability': 'NEGLIGIBLE', 'blocked': False}]}, id='run-e905f4f4-58cb-4a10-a960-448a2bb649e3', usage_metadata={'input_tokens': 18, 'output_tokens': 5, 'total_tokens': 23})\n",
    "\n",
    "        .. code-block:: python\n",
    "\n",
    "            stream = llm.stream(messages)\n",
    "            full = next(stream)\n",
    "            for chunk in stream:\n",
    "                full += chunk\n",
    "            full\n",
    "\n",
    "        .. code-block:: python\n",
    "\n",
    "            AIMessageChunk(\n",
    "                content=\"J'adore programmer. \\\\n\",\n",
    "                response_metadata={'finish_reason': 'STOPSTOP', 'safety_ratings': [{'category': 'HARM_CATEGORY_SEXUALLY_EXPLICIT', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_HATE_SPEECH', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_HARASSMENT', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_DANGEROUS_CONTENT', 'probability': 'NEGLIGIBLE', 'blocked': False}]},\n",
    "                id='run-3ce13a42-cd30-4ad7-a684-f1f0b37cdeec',\n",
    "                usage_metadata={'input_tokens': 36, 'output_tokens': 6, 'total_tokens': 42}\n",
    "            )\n",
    "\n",
    "    Async:\n",
    "        .. code-block:: python\n",
    "\n",
    "            await llm.ainvoke(messages)\n",
    "\n",
    "            # stream:\n",
    "            # async for chunk in (await llm.astream(messages))\n",
    "\n",
    "            # batch:\n",
    "            # await llm.abatch([messages])\n",
    "\n",
    "    Tool calling:\n",
    "        .. code-block:: python\n",
    "\n",
    "            from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "\n",
    "\n",
    "            class GetWeather(BaseModel):\n",
    "                '''Get the current weather in a given location'''\n",
    "\n",
    "                location: str = Field(\n",
    "                    ..., description=\"The city and state, e.g. San Francisco, CA\"\n",
    "                )\n",
    "\n",
    "\n",
    "            class GetPopulation(BaseModel):\n",
    "                '''Get the current population in a given location'''\n",
    "\n",
    "                location: str = Field(\n",
    "                    ..., description=\"The city and state, e.g. San Francisco, CA\"\n",
    "                )\n",
    "\n",
    "\n",
    "            llm_with_tools = llm.bind_tools([GetWeather, GetPopulation])\n",
    "            ai_msg = llm_with_tools.invoke(\n",
    "                \"Which city is hotter today and which is bigger: LA or NY?\"\n",
    "            )\n",
    "            ai_msg.tool_calls\n",
    "\n",
    "        .. code-block:: python\n",
    "\n",
    "            [{'name': 'GetWeather',\n",
    "              'args': {'location': 'Los Angeles, CA'},\n",
    "              'id': 'c186c99f-f137-4d52-947f-9e3deabba6f6'},\n",
    "             {'name': 'GetWeather',\n",
    "              'args': {'location': 'New York City, NY'},\n",
    "              'id': 'cebd4a5d-e800-4fa5-babd-4aa286af4f31'},\n",
    "             {'name': 'GetPopulation',\n",
    "              'args': {'location': 'Los Angeles, CA'},\n",
    "              'id': '4f92d897-f5e4-4d34-a3bc-93062c92591e'},\n",
    "             {'name': 'GetPopulation',\n",
    "              'args': {'location': 'New York City, NY'},\n",
    "              'id': '634582de-5186-4e4b-968b-f192f0a93678'}]\n",
    "\n",
    "    Structured output:\n",
    "        .. code-block:: python\n",
    "\n",
    "            from typing import Optional\n",
    "\n",
    "            from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "\n",
    "\n",
    "            class Joke(BaseModel):\n",
    "                '''Joke to tell user.'''\n",
    "\n",
    "                setup: str = Field(description=\"The setup of the joke\")\n",
    "                punchline: str = Field(description=\"The punchline to the joke\")\n",
    "                rating: Optional[int] = Field(description=\"How funny the joke is, from 1 to 10\")\n",
    "\n",
    "\n",
    "            structured_llm = llm.with_structured_output(Joke)\n",
    "            structured_llm.invoke(\"Tell me a joke about cats\")\n",
    "\n",
    "        .. code-block:: python\n",
    "\n",
    "            Joke(\n",
    "                setup='Why are cats so good at video games?',\n",
    "                punchline='They have nine lives on the internet',\n",
    "                rating=None\n",
    "            )\n",
    "\n",
    "    Image input:\n",
    "        .. code-block:: python\n",
    "\n",
    "            import base64\n",
    "            import httpx\n",
    "            from langchain_core.messages import HumanMessage\n",
    "\n",
    "            image_url = \"https://upload.wikimedia.org/wikipedia/commons/thumb/d/dd/Gfp-wisconsin-madison-the-nature-boardwalk.jpg/2560px-Gfp-wisconsin-madison-the-nature-boardwalk.jpg\"\n",
    "            image_data = base64.b64encode(httpx.get(image_url).content).decode(\"utf-8\")\n",
    "            message = HumanMessage(\n",
    "                content=[\n",
    "                    {\"type\": \"text\", \"text\": \"describe the weather in this image\"},\n",
    "                    {\n",
    "                        \"type\": \"image_url\",\n",
    "                        \"image_url\": {\"url\": f\"data:image/jpeg;base64,{image_data}\"},\n",
    "                    },\n",
    "                ]\n",
    "            )\n",
    "            ai_msg = llm.invoke([message])\n",
    "            ai_msg.content\n",
    "\n",
    "        .. code-block:: python\n",
    "\n",
    "            'The weather in this image appears to be sunny and pleasant. The sky is a bright blue with scattered white clouds, suggesting fair weather. The lush green grass and trees indicate a warm and possibly slightly breezy day. There are no signs of rain or storms. \\n'\n",
    "\n",
    "    Token usage:\n",
    "        .. code-block:: python\n",
    "\n",
    "            ai_msg = llm.invoke(messages)\n",
    "            ai_msg.usage_metadata\n",
    "\n",
    "        .. code-block:: python\n",
    "\n",
    "            {'input_tokens': 18, 'output_tokens': 5, 'total_tokens': 23}\n",
    "\n",
    "\n",
    "        Response metadata\n",
    "        .. code-block:: python\n",
    "\n",
    "            ai_msg = llm.invoke(messages)\n",
    "            ai_msg.response_metadata\n",
    "\n",
    "        .. code-block:: python\n",
    "\n",
    "            {\n",
    "                'prompt_feedback': {'block_reason': 0, 'safety_ratings': []},\n",
    "                'finish_reason': 'STOP',\n",
    "                'safety_ratings': [{'category': 'HARM_CATEGORY_SEXUALLY_EXPLICIT', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_HATE_SPEECH', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_HARASSMENT', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_DANGEROUS_CONTENT', 'probability': 'NEGLIGIBLE', 'blocked': False}]\n",
    "            }\n",
    "\n",
    "    \"\"\"  # noqa: E501\n",
    "\n",
    "    client: Any = None  #: :meta private:\n",
    "    async_client: Any = None  #: :meta private:\n",
    "    google_api_key: Optional[SecretStr] = Field(default=None, alias=\"api_key\")\n",
    "    \"\"\"Google AI API key. \n",
    "        \n",
    "    If not specified will be read from env var ``GOOGLE_API_KEY``.\"\"\"\n",
    "    default_metadata: Sequence[Tuple[str, str]] = Field(\n",
    "        default_factory=list\n",
    "    )  #: :meta private:\n",
    "\n",
    "    convert_system_message_to_human: bool = False\n",
    "    \"\"\"Whether to merge any leading SystemMessage into the following HumanMessage.\n",
    "    \n",
    "    Gemini does not support system messages; any unsupported messages will \n",
    "    raise an error.\"\"\"\n",
    "\n",
    "    class Config:\n",
    "        allow_population_by_field_name = True\n",
    "\n",
    "    @property\n",
    "    def lc_secrets(self) -> Dict[str, str]:\n",
    "        return {\"google_api_key\": \"GOOGLE_API_KEY\"}\n",
    "\n",
    "    @property\n",
    "    def _llm_type(self) -> str:\n",
    "        return \"chat-google-generative-ai\"\n",
    "\n",
    "    @classmethod\n",
    "    def is_lc_serializable(self) -> bool:\n",
    "        return True\n",
    "\n",
    "    @root_validator()\n",
    "    def validate_environment(cls, values: Dict) -> Dict:\n",
    "        \"\"\"Validates params and passes them to google-generativeai package.\"\"\"\n",
    "        if (\n",
    "            values.get(\"temperature\") is not None\n",
    "            and not 0 <= values[\"temperature\"] <= 1\n",
    "        ):\n",
    "            raise ValueError(\"temperature must be in the range [0.0, 1.0]\")\n",
    "\n",
    "        if values.get(\"top_p\") is not None and not 0 <= values[\"top_p\"] <= 1:\n",
    "            raise ValueError(\"top_p must be in the range [0.0, 1.0]\")\n",
    "\n",
    "        if values.get(\"top_k\") is not None and values[\"top_k\"] <= 0:\n",
    "            raise ValueError(\"top_k must be positive\")\n",
    "\n",
    "        if not values[\"model\"].startswith(\"models/\"):\n",
    "            values[\"model\"] = f\"models/{values['model']}\"\n",
    "\n",
    "        additional_headers = values.get(\"additional_headers\") or {}\n",
    "        values[\"default_metadata\"] = tuple(additional_headers.items())\n",
    "        client_info = get_client_info(\"ChatGoogleGenerativeAI\")\n",
    "        google_api_key = None\n",
    "        if not values.get(\"credentials\"):\n",
    "            google_api_key = get_from_dict_or_env(\n",
    "                values, \"google_api_key\", \"GOOGLE_API_KEY\"\n",
    "            )\n",
    "            if isinstance(google_api_key, SecretStr):\n",
    "                google_api_key = google_api_key.get_secret_value()\n",
    "        transport: Optional[str] = values.get(\"transport\")\n",
    "        values[\"client\"] = genaix.build_generative_service(\n",
    "            credentials=values.get(\"credentials\"),\n",
    "            api_key=google_api_key,\n",
    "            client_info=client_info,\n",
    "            client_options=values.get(\"client_options\"),\n",
    "            transport=transport,\n",
    "        )\n",
    "\n",
    "        # NOTE: genaix.build_generative_async_service requires\n",
    "        # a running event loop, which causes an error\n",
    "        # when initialized inside a ThreadPoolExecutor.\n",
    "        # this check ensures that async client is only initialized\n",
    "        # within an asyncio event loop to avoid the error\n",
    "        if _is_event_loop_running():\n",
    "            values[\"async_client\"] = genaix.build_generative_async_service(\n",
    "                credentials=values.get(\"credentials\"),\n",
    "                api_key=google_api_key,\n",
    "                client_info=client_info,\n",
    "                client_options=values.get(\"client_options\"),\n",
    "                transport=transport,\n",
    "            )\n",
    "        else:\n",
    "            values[\"async_client\"] = None\n",
    "\n",
    "        return values\n",
    "\n",
    "    @property\n",
    "    def _identifying_params(self) -> Dict[str, Any]:\n",
    "        \"\"\"Get the identifying parameters.\"\"\"\n",
    "        return {\n",
    "            \"model\": self.model,\n",
    "            \"temperature\": self.temperature,\n",
    "            \"top_k\": self.top_k,\n",
    "            \"n\": self.n,\n",
    "            \"safety_settings\": self.safety_settings,\n",
    "        }\n",
    "\n",
    "    def _get_ls_params(\n",
    "        self, stop: Optional[List[str]] = None, **kwargs: Any\n",
    "    ) -> LangSmithParams:\n",
    "        \"\"\"Get standard params for tracing.\"\"\"\n",
    "        params = self._get_invocation_params(stop=stop, **kwargs)\n",
    "        ls_params = LangSmithParams(\n",
    "            ls_provider=\"google_genai\",\n",
    "            ls_model_name=self.model,\n",
    "            ls_model_type=\"chat\",\n",
    "            ls_temperature=params.get(\"temperature\", self.temperature),\n",
    "        )\n",
    "        if ls_max_tokens := params.get(\"max_output_tokens\", self.max_output_tokens):\n",
    "            ls_params[\"ls_max_tokens\"] = ls_max_tokens\n",
    "        if ls_stop := stop or params.get(\"stop\", None):\n",
    "            ls_params[\"ls_stop\"] = ls_stop\n",
    "        return ls_params\n",
    "\n",
    "    def _prepare_params(\n",
    "        self,\n",
    "        stop: Optional[List[str]],\n",
    "        generation_config: Optional[Dict[str, Any]] = None,\n",
    "    ) -> GenerationConfig:\n",
    "        gen_config = {\n",
    "            k: v\n",
    "            for k, v in {\n",
    "                \"candidate_count\": self.n,\n",
    "                \"temperature\": self.temperature,\n",
    "                \"stop_sequences\": stop,\n",
    "                \"max_output_tokens\": self.max_output_tokens,\n",
    "                \"top_k\": self.top_k,\n",
    "                \"top_p\": self.top_p,\n",
    "            }.items()\n",
    "            if v is not None\n",
    "        }\n",
    "        if generation_config:\n",
    "            gen_config = {**gen_config, **generation_config}\n",
    "        return GenerationConfig(**gen_config)\n",
    "\n",
    "    def _generate(\n",
    "        self,\n",
    "        messages: List[BaseMessage],\n",
    "        stop: Optional[List[str]] = None,\n",
    "        run_manager: Optional[CallbackManagerForLLMRun] = None,\n",
    "        *,\n",
    "        tools: Optional[Sequence[Union[ToolDict, GoogleTool]]] = None,\n",
    "        functions: Optional[Sequence[FunctionDeclarationType]] = None,\n",
    "        safety_settings: Optional[SafetySettingDict] = None,\n",
    "        tool_config: Optional[Union[Dict, _ToolConfigDict]] = None,\n",
    "        generation_config: Optional[Dict[str, Any]] = None,\n",
    "        **kwargs: Any,\n",
    "    ) -> ChatResult:\n",
    "        request = self._prepare_request(\n",
    "            messages,\n",
    "            stop=stop,\n",
    "            tools=tools,\n",
    "            functions=functions,\n",
    "            safety_settings=safety_settings,\n",
    "            tool_config=tool_config,\n",
    "            generation_config=generation_config,\n",
    "        )\n",
    "        response: GenerateContentResponse = _chat_with_retry(\n",
    "            request=request,\n",
    "            **kwargs,\n",
    "            generation_method=self.client.generate_content,\n",
    "            metadata=self.default_metadata,\n",
    "        )\n",
    "        return _response_to_result(response)\n",
    "\n",
    "    async def _agenerate(\n",
    "        self,\n",
    "        messages: List[BaseMessage],\n",
    "        stop: Optional[List[str]] = None,\n",
    "        run_manager: Optional[AsyncCallbackManagerForLLMRun] = None,\n",
    "        *,\n",
    "        tools: Optional[Sequence[Union[ToolDict, GoogleTool]]] = None,\n",
    "        functions: Optional[Sequence[FunctionDeclarationType]] = None,\n",
    "        safety_settings: Optional[SafetySettingDict] = None,\n",
    "        tool_config: Optional[Union[Dict, _ToolConfigDict]] = None,\n",
    "        generation_config: Optional[Dict[str, Any]] = None,\n",
    "        **kwargs: Any,\n",
    "    ) -> ChatResult:\n",
    "        if not self.async_client:\n",
    "            updated_kwargs = {\n",
    "                **kwargs,\n",
    "                **{\n",
    "                    \"tools\": tools,\n",
    "                    \"functions\": functions,\n",
    "                    \"safety_settings\": safety_settings,\n",
    "                    \"tool_config\": tool_config,\n",
    "                    \"generation_config\": generation_config,\n",
    "                },\n",
    "            }\n",
    "            return await super()._agenerate(\n",
    "                messages, stop, run_manager, **updated_kwargs\n",
    "            )\n",
    "\n",
    "        request = self._prepare_request(\n",
    "            messages,\n",
    "            stop=stop,\n",
    "            tools=tools,\n",
    "            functions=functions,\n",
    "            safety_settings=safety_settings,\n",
    "            tool_config=tool_config,\n",
    "            generation_config=generation_config,\n",
    "        )\n",
    "        response: GenerateContentResponse = await _achat_with_retry(\n",
    "            request=request,\n",
    "            **kwargs,\n",
    "            generation_method=self.async_client.generate_content,\n",
    "            metadata=self.default_metadata,\n",
    "        )\n",
    "        return _response_to_result(response)\n",
    "\n",
    "    def _stream(\n",
    "        self,\n",
    "        messages: List[BaseMessage],\n",
    "        stop: Optional[List[str]] = None,\n",
    "        run_manager: Optional[CallbackManagerForLLMRun] = None,\n",
    "        *,\n",
    "        tools: Optional[Sequence[Union[ToolDict, GoogleTool]]] = None,\n",
    "        functions: Optional[Sequence[FunctionDeclarationType]] = None,\n",
    "        safety_settings: Optional[SafetySettingDict] = None,\n",
    "        tool_config: Optional[Union[Dict, _ToolConfigDict]] = None,\n",
    "        generation_config: Optional[Dict[str, Any]] = None,\n",
    "        **kwargs: Any,\n",
    "    ) -> Iterator[ChatGenerationChunk]:\n",
    "        request = self._prepare_request(\n",
    "            messages,\n",
    "            stop=stop,\n",
    "            tools=tools,\n",
    "            functions=functions,\n",
    "            safety_settings=safety_settings,\n",
    "            tool_config=tool_config,\n",
    "            generation_config=generation_config,\n",
    "        )\n",
    "        response: GenerateContentResponse = _chat_with_retry(\n",
    "            request=request,\n",
    "            generation_method=self.client.stream_generate_content,\n",
    "            **kwargs,\n",
    "            metadata=self.default_metadata,\n",
    "        )\n",
    "        for chunk in response:\n",
    "            _chat_result = _response_to_result(chunk, stream=True)\n",
    "            gen = cast(ChatGenerationChunk, _chat_result.generations[0])\n",
    "\n",
    "            if run_manager:\n",
    "                run_manager.on_llm_new_token(gen.text)\n",
    "            yield gen\n",
    "\n",
    "    async def _astream(\n",
    "        self,\n",
    "        messages: List[BaseMessage],\n",
    "        stop: Optional[List[str]] = None,\n",
    "        run_manager: Optional[AsyncCallbackManagerForLLMRun] = None,\n",
    "        *,\n",
    "        tools: Optional[Sequence[Union[ToolDict, GoogleTool]]] = None,\n",
    "        functions: Optional[Sequence[FunctionDeclarationType]] = None,\n",
    "        safety_settings: Optional[SafetySettingDict] = None,\n",
    "        tool_config: Optional[Union[Dict, _ToolConfigDict]] = None,\n",
    "        generation_config: Optional[Dict[str, Any]] = None,\n",
    "        **kwargs: Any,\n",
    "    ) -> AsyncIterator[ChatGenerationChunk]:\n",
    "        if not self.async_client:\n",
    "            updated_kwargs = {\n",
    "                **kwargs,\n",
    "                **{\n",
    "                    \"tools\": tools,\n",
    "                    \"functions\": functions,\n",
    "                    \"safety_settings\": safety_settings,\n",
    "                    \"tool_config\": tool_config,\n",
    "                    \"generation_config\": generation_config,\n",
    "                },\n",
    "            }\n",
    "            async for value in super()._astream(\n",
    "                messages, stop, run_manager, **updated_kwargs\n",
    "            ):\n",
    "                yield value\n",
    "        else:\n",
    "            request = self._prepare_request(\n",
    "                messages,\n",
    "                stop=stop,\n",
    "                tools=tools,\n",
    "                functions=functions,\n",
    "                safety_settings=safety_settings,\n",
    "                tool_config=tool_config,\n",
    "                generation_config=generation_config,\n",
    "            )\n",
    "            async for chunk in await _achat_with_retry(\n",
    "                request=request,\n",
    "                generation_method=self.async_client.stream_generate_content,\n",
    "                **kwargs,\n",
    "                metadata=self.default_metadata,\n",
    "            ):\n",
    "                _chat_result = _response_to_result(chunk, stream=True)\n",
    "                gen = cast(ChatGenerationChunk, _chat_result.generations[0])\n",
    "\n",
    "                if run_manager:\n",
    "                    await run_manager.on_llm_new_token(gen.text)\n",
    "                yield gen\n",
    "\n",
    "    def _prepare_request(\n",
    "        self,\n",
    "        messages: List[BaseMessage],\n",
    "        *,\n",
    "        stop: Optional[List[str]] = None,\n",
    "        tools: Optional[Sequence[Union[ToolDict, GoogleTool]]] = None,\n",
    "        functions: Optional[Sequence[FunctionDeclarationType]] = None,\n",
    "        safety_settings: Optional[SafetySettingDict] = None,\n",
    "        tool_config: Optional[Union[Dict, _ToolConfigDict]] = None,\n",
    "        generation_config: Optional[Dict[str, Any]] = None,\n",
    "    ) -> Tuple[GenerateContentRequest, Dict[str, Any]]:\n",
    "        formatted_tools = None\n",
    "        if tools:\n",
    "            formatted_tools = [convert_to_genai_function_declarations(tools)]\n",
    "        elif functions:\n",
    "            formatted_tools = [convert_to_genai_function_declarations(functions)]\n",
    "\n",
    "        system_instruction, history = _parse_chat_history(\n",
    "            messages,\n",
    "            convert_system_message_to_human=self.convert_system_message_to_human,\n",
    "        )\n",
    "        formatted_tool_config = None\n",
    "        if tool_config:\n",
    "            formatted_tool_config = ToolConfig(\n",
    "                function_calling_config=tool_config[\"function_calling_config\"]\n",
    "            )\n",
    "        formatted_safety_settings = []\n",
    "        if safety_settings:\n",
    "            formatted_safety_settings = [\n",
    "                SafetySetting(category=c, threshold=t)\n",
    "                for c, t in safety_settings.items()\n",
    "            ]\n",
    "        request = GenerateContentRequest(\n",
    "            model=self.model,\n",
    "            contents=history,\n",
    "            tools=formatted_tools,\n",
    "            tool_config=formatted_tool_config,\n",
    "            safety_settings=formatted_safety_settings,\n",
    "            generation_config=self._prepare_params(\n",
    "                stop, generation_config=generation_config\n",
    "            ),\n",
    "        )\n",
    "        if system_instruction:\n",
    "            request.system_instruction = system_instruction\n",
    "\n",
    "        return request\n",
    "\n",
    "    def get_num_tokens(self, text: str) -> int:\n",
    "        \"\"\"Get the number of tokens present in the text.\n",
    "\n",
    "        Useful for checking if an input will fit in a model's context window.\n",
    "\n",
    "        Args:\n",
    "            text: The string input to tokenize.\n",
    "\n",
    "        Returns:\n",
    "            The integer number of tokens in the text.\n",
    "        \"\"\"\n",
    "        result = self.client.count_tokens(\n",
    "            model=self.model, contents=[Content(parts=[Part(text=text)])]\n",
    "        )\n",
    "        return result.total_tokens\n",
    "\n",
    "    def with_structured_output(\n",
    "        self,\n",
    "        schema: Union[Dict, Type[BaseModel]],\n",
    "        *,\n",
    "        include_raw: bool = False,\n",
    "        **kwargs: Any,\n",
    "    ) -> Runnable[LanguageModelInput, Union[Dict, BaseModel]]:\n",
    "        if kwargs:\n",
    "            \n",
    "            raise ValueError(f\"Received unsupported arguments {kwargs}\")\n",
    "        if isinstance(schema, type) and issubclass(schema, BaseModel):\n",
    "            parser: OutputParserLike = PydanticToolsParser(\n",
    "                tools=[schema], first_tool_only=True\n",
    "            )\n",
    "        else:\n",
    "            parser = JsonOutputToolsParser()\n",
    "        tool_choice = _get_tool_name(schema) if self._supports_tool_choice else None\n",
    "        llm = self.bind_tools([schema], tool_choice=tool_choice)\n",
    "                \n",
    "        if include_raw:\n",
    "            parser_with_fallback = RunnablePassthrough.assign(\n",
    "                parsed=itemgetter(\"raw\") | parser, parsing_error=lambda _: None\n",
    "            ).with_fallbacks(\n",
    "                [RunnablePassthrough.assign(parsed=lambda _: None)],\n",
    "                exception_key=\"parsing_error\",\n",
    "            )\n",
    "            return {\"raw\": llm} | parser_with_fallback\n",
    "        else:\n",
    "            return llm | parser\n",
    "\n",
    "    def bind_tools(\n",
    "        self,\n",
    "        tools: Sequence[Union[ToolDict, GoogleTool]],\n",
    "        tool_config: Optional[Union[Dict, _ToolConfigDict]] = None,\n",
    "        *,\n",
    "        tool_choice: Optional[Union[_ToolChoiceType, bool]] = None,\n",
    "        **kwargs: Any,\n",
    "    ) -> Runnable[LanguageModelInput, BaseMessage]:\n",
    "        \"\"\"Bind tool-like objects to this chat model.\n",
    "\n",
    "        Assumes model is compatible with google-generativeAI tool-calling API.\n",
    "\n",
    "        Args:\n",
    "            tools: A list of tool definitions to bind to this chat model.\n",
    "                Can be a pydantic model, callable, or BaseTool. Pydantic\n",
    "                models, callables, and BaseTools will be automatically converted to\n",
    "                their schema dictionary representation.\n",
    "            **kwargs: Any additional parameters to pass to the\n",
    "                :class:`~langchain.runnable.Runnable` constructor.\n",
    "        \"\"\"\n",
    "        if tool_choice and tool_config:\n",
    "            raise ValueError(\n",
    "                \"Must specify at most one of tool_choice and tool_config, received \"\n",
    "                f\"both:\\n\\n{tool_choice=}\\n\\n{tool_config=}\"\n",
    "            )\n",
    "        # Bind dicts for easier serialization/deserialization.\n",
    "        genai_tools = [tool_to_dict(convert_to_genai_function_declarations(tools))]\n",
    "        if tool_choice:\n",
    "            all_names = [\n",
    "                f[\"name\"]  # type: ignore[index]\n",
    "                for t in genai_tools\n",
    "                for f in t[\"function_declarations\"]\n",
    "            ]\n",
    "            tool_config = _tool_choice_to_tool_config(tool_choice, all_names)\n",
    "        return self.bind(tools=genai_tools, tool_config=tool_config, **kwargs)\n",
    "\n",
    "    @property\n",
    "    def _supports_tool_choice(self) -> bool:\n",
    "        return \"gemini-1.5-pro\" in self.model\n",
    "    \n",
    "def _get_tool_name(\n",
    "    tool: Union[ToolDict, GoogleTool],\n",
    ") -> str:\n",
    "    genai_tool = tool_to_dict(convert_to_genai_function_declarations([tool]))\n",
    "    r = [f[\"name\"] for f in genai_tool[\"function_declarations\"]][0]\n",
    "\n",
    "    # print('genai_tool[\"function_declarations\"]')\n",
    "    # print(genai_tool[\"function_declarations\"])\n",
    "\n",
    "    return r\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------\n",
      "tool\n",
      "function_declarations {\n",
      "  name: \"QuestionList\"\n",
      "  description: \"A list of Question class.\"\n",
      "  parameters {\n",
      "    type_: OBJECT\n",
      "    properties {\n",
      "      key: \"question_list\"\n",
      "      value {\n",
      "        type_: ARRAY\n",
      "        description: \"list of Question\"\n",
      "      }\n",
      "    }\n",
      "    required: \"question_list\"\n",
      "  }\n",
      "}\n",
      "\n",
      "<class 'google.ai.generativelanguage_v1beta.types.content.Tool'>\n",
      "---------------------------\n",
      "type(tool).to_dict(tool)\n",
      "{'function_declarations': [{'name': 'QuestionList', 'description': 'A list of Question class.', 'parameters': {'type_': 6, 'properties': {'question_list': {'type_': 5, 'description': 'list of Question', 'format_': '', 'nullable': False, 'enum': [], 'properties': {}, 'required': []}}, 'required': ['question_list'], 'format_': '', 'description': '', 'nullable': False, 'enum': []}}]}\n",
      "---------------------------\n",
      "type(t)\n",
      "<class 'dict'>\n",
      "---------------------------\n",
      "---------------------------\n",
      "tool\n",
      "function_declarations {\n",
      "  name: \"QuestionList\"\n",
      "  description: \"A list of Question class.\"\n",
      "  parameters {\n",
      "    type_: OBJECT\n",
      "    properties {\n",
      "      key: \"question_list\"\n",
      "      value {\n",
      "        type_: ARRAY\n",
      "        description: \"list of Question\"\n",
      "      }\n",
      "    }\n",
      "    required: \"question_list\"\n",
      "  }\n",
      "}\n",
      "\n",
      "<class 'google.ai.generativelanguage_v1beta.types.content.Tool'>\n",
      "---------------------------\n",
      "type(tool).to_dict(tool)\n",
      "{'function_declarations': [{'name': 'QuestionList', 'description': 'A list of Question class.', 'parameters': {'type_': 6, 'properties': {'question_list': {'type_': 5, 'description': 'list of Question', 'format_': '', 'nullable': False, 'enum': [], 'properties': {}, 'required': []}}, 'required': ['question_list'], 'format_': '', 'description': '', 'nullable': False, 'enum': []}}]}\n",
      "---------------------------\n",
      "type(t)\n",
      "<class 'dict'>\n",
      "---------------------------\n"
     ]
    }
   ],
   "source": [
    "# llm_google = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash\")\n",
    "llm_google = ChatGoogleGenerativeAI(model=\"gemini-1.5-pro\")\n",
    "llm_google_QuestionList = llm_google.with_structured_output(QuestionList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_question_generator = PromptTemplate.from_template(\"\"\"\n",
    "\n",
    "    TASK CONTEXT:\n",
    "    I am studying machine learning and I need to practice some questions on various topics.\n",
    "    \n",
    "    TASK DESCRIPTION:\n",
    "    I will provide you with a list of topics, and I would like you to generate a list of TRUE or FALSE questions.\n",
    "    These questions should be interesting, creative, challenging and thought-provoking. \n",
    "    Each question should be in the form of a statement that could be either TRUE or FALSE.\n",
    "    Feel free to be imaginative and attempt to confuse the student by blending related concepts or similar words.\n",
    "    I will provide the topics in the DOMAIN KNOWLEDGE section.\n",
    "    The questions should pertain to these topics, and you can use this knowledge as a foundation to create questions that delve deeper into the subject matter.\n",
    "    \n",
    "    ADDITIONAL TASK DESCRIPTION:\n",
    "    {additional_task_description}\n",
    "    \n",
    "    TASK REQUIREMENTS:\n",
    "    Please refrain from creating questions that require mathematical calculations, but you may create questions with mathematical formulas.\n",
    "    You SHOULD use LATEX to write mathematical formulas and code, but you should use the Katex flavor.\n",
    "    Also you should put $$ in the beggining of the katex code and $$ at the end of the code. This is necessary because the interpreter needs it.\n",
    "    \n",
    "    TASK DETAILS:\n",
    "    You should create {quantity} questions of level {level}.\n",
    "    \n",
    "    DOMAIN KNOWLEDGE:\n",
    "    {domain_knowledge}\n",
    "    \n",
    "    FORMAT OUTPUT INSTRUCTIONS:\n",
    "    It should be formatted as described in the output format.\n",
    "    \"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from supabase import create_client, Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "url: str = os.environ.get(\"SUPABASE_URL\")\n",
    "key: str = os.environ.get(\"SUPABASE_KEY\")\n",
    "supabase: Client = create_client(url, key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"\n",
    "\n",
    "Language Models and Probability\n",
    "Entropy\n",
    "Cross-Entropy\n",
    "Cross-Entropy Loss\n",
    "Perplexity\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_questions(text, llm, parameters, subject_matter_1, subject_matter_2):\n",
    "    print( \"------------------- generate_questions FUNCTION -------------------\" )\n",
    "    \n",
    "    if text is None:\n",
    "        raise Exception(\"text is None\")\n",
    "        \n",
    "    try:\n",
    "        chain = prompt_question_generator | llm\n",
    "        \n",
    "        parameters[\"domain_knowledge\"] = text\n",
    "        \n",
    "        questions = chain.invoke(parameters)\n",
    "\n",
    "        print(questions)\n",
    "                \n",
    "        # for q in questions:\n",
    "        #     q[\"subject_matter_1\"] = subject_matter_1\n",
    "        #     q[\"subject_matter_2\"] = subject_matter_2    \n",
    "        \n",
    "        # data, count = supabase.table('questions').insert(questions).execute()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(\"An error occurred:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "level: hard\n",
      "------------------- generate_questions FUNCTION -------------------\n",
      "An error occurred: Invalid argument provided to Gemini: 400 * GenerateContentRequest.tools[0].function_declarations[0].parameters.properties[question_list].items: missing field.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for level in [\n",
    "    # \"beginner\", \n",
    "    # \"intermediate\", \n",
    "    \"hard\"\n",
    "    ]:\n",
    "    print( \"level:\", level )\n",
    "    \n",
    "    parameters = {\n",
    "        \"quantity\": 1,\n",
    "        \"level\": level,\n",
    "        \"additional_task_description\": \"Create questions only about the definitions of the concepts, like mixing the definition of one with another distribution, or mixing the use of one with the use of another. I need this to memorize this concepts.\"\n",
    "    }\n",
    "    \n",
    "    subject_matter_1 = \"Probability - LLMs\"\n",
    "    subject_matter_2 = subject_matter_1\n",
    "    \n",
    "    generate_questions(text, llm_google_QuestionList, parameters, subject_matter_1, subject_matter_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
