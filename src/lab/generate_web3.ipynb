{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rhuan/PROJETOS/quiz_proj2/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# vector database\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "from langchain_community.vectorstores import Qdrant\n",
    "\n",
    "# ingestion\n",
    "from langchain_core.documents import Document\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter, CharacterTextSplitter\n",
    "\n",
    "# from llama_index.core import SimpleDirectoryReader\n",
    "\n",
    "# chat\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.messages import AIMessage, HumanMessage\n",
    "from langchain_core.messages.base import BaseMessage\n",
    "\n",
    "from langchain.prompts import FewShotPromptTemplate, PromptTemplate\n",
    "\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "\n",
    "from pydantic import BaseModel\n",
    "from typing import List, Optional\n",
    "from langchain.output_parsers import PydanticOutputParser\n",
    "import json\n",
    "import re\n",
    "\n",
    "# system\n",
    "import os\n",
    "import logging\n",
    "import sys\n",
    "\n",
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "logging.basicConfig(level=logging.DEBUG,  # Define o nível de log\n",
    "                    format='%(asctime)s - %(levelname)s - %(message)s',  # Define o formato da mensagem de log\n",
    "                    stream=sys.stdout)  # Define a saída do log para stdout\n",
    "                    # filename='app.log',  # Define o arquivo onde os logs serão gravados\n",
    "                    # filemode='a')  # Define o modo de escrita do arquivo de log (append)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = \"quiz\"\n",
    "os.environ[\"LANGCHAIN_ENDPOINT\"] = \"https://api.smith.langchain.com\"\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = \"ls__b626f8e0970e43cca449e7a3510ac96b\"  # Update to your API key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.info('Inicializando LLM e embedings')\n",
    "api_key_google = \"AIzaSyC-V6lfROehy46ntB6zPZ7CJ8zNF3gDdO4\"\n",
    "llm_google = ChatGoogleGenerativeAI(model=\"gemini-pro\", convert_system_message_to_human=True, google_api_key=api_key_google)\n",
    "embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\", google_api_key=api_key_google)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm_openai = ChatOpenAI(openai_api_key=\"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\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "object_schema = \"\"\"\n",
    "{\n",
    "  \"properties\": {\n",
    "    \"topic_description\": {\n",
    "      \"type\": \"string\",\n",
    "      \"description\": \"A sentence describing the sub-topic to which the question belongs. That means this sentence should specify in a granular level what specific sub-topic the question belongs to. It should be abstract in a way that other questions could be put in this description too. Use between 5 and 10 words.\"\n",
    "    },\n",
    "    \"level\": {\n",
    "      \"type\": \"string\",\n",
    "      \"description\": \"The difficulty level of the question. It should be only one of the following options: 'beginner', 'intermediate', 'advanced'.\"\n",
    "    },\n",
    "    \"question\": {\n",
    "      \"type\": \"string\",\n",
    "      \"description\": \"The actual question text. It should be a question of type TRUE or FALSE. It means that the questions should be an assertion that could be answered with TRUE or FALSE.\"\n",
    "    },\n",
    "    \"answer\": {\n",
    "      \"type\": \"string\",\n",
    "      \"description\": \"The correct answer to the question. It should be only one of the following options: TRUE or FALSE\"\n",
    "    },\n",
    "    \"explanation\": {\n",
    "      \"type\": \"string\",\n",
    "      \"description\": \"An explanation or solution to the question.\"\n",
    "    }\n",
    "  },\n",
    "  \"required\": [\"topic_description\", \"level\", \"question\", \"answer\", \"explanation\"]\n",
    "}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_question_generator = PromptTemplate(\n",
    "    template=\"\"\"\n",
    "                TASK CONTEXT:\n",
    "                I am studying machine learning and I need to practice some questions on various topics.\n",
    "                \n",
    "                TASK DESCRIPTION:\n",
    "                I will provide you with a list of topics, and I would like you to generate a list of TRUE or FALSE questions.\n",
    "                These questions should be interesting, creative, challenging and thought-provoking. \n",
    "                Each question should be in the form of a statement that could be either TRUE or FALSE.\n",
    "                Feel free to be imaginative and attempt to confuse the student by blending related concepts or similar words.\n",
    "                I will provide the topics in the DOMAIN KNOWLEDGE section.\n",
    "                The questions should pertain to these topics, and you can use this knowledge as a foundation to create questions that delve deeper into the subject matter.\n",
    "                \n",
    "                TASK REQUIREMENTS:\n",
    "                Please refrain from creating questions that require mathematical calculations, but you may create questions with mathematical formulas.\n",
    "                You SHOULD use LATEX to write mathematical formulas and code, but you should use the Katex flavor.\n",
    "                Also you should put $$ in the beggining of the katex code and $$ at the end of the code. This is necessary because the interpreter needs it.\n",
    "                \n",
    "                TASK DETAILS:\n",
    "                You should create {quantity} questions of level {level}.\n",
    "                \n",
    "                DOMAIN KNOWLEDGE:\n",
    "                {domain_knowledge}\n",
    "                \n",
    "                FORMAT OUTPUT INSTRUCTIONS:\n",
    "                The output should be formatted as a JSON list of objects that conforms class object schema below.\n",
    "                You should output just the Json list. \n",
    "                You should not output any other word like \"json\" in the beginning because it will ruin the parser.\n",
    "\n",
    "                ```\n",
    "                {object_schema}\n",
    "                ```\n",
    "            \"\"\",\n",
    "    input_variables=[\"quantity\", \"level\"],\n",
    "    partial_variables={\"object_schema\": object_schema},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from supabase import create_client, Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://xoxlgvakygiyfijfeixu.supabase.co\"\n",
    "key = \"eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJzdXBhYmFzZSIsInJlZiI6InhveGxndmFreWdpeWZpamZlaXh1Iiwicm9sZSI6InNlcnZpY2Vfcm9sZSIsImlhdCI6MTcwNDkyNzU2NywiZXhwIjoyMDIwNTAzNTY3fQ.V3766GRj6hkt1Ci-52tjSiULVoF3nfCPPDnR6Hc_rT0\"\n",
    "\n",
    "supabase: Client = create_client(url, key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import  json_repair\n",
    "\n",
    "def json_parser(message: AIMessage) -> List[dict]:\n",
    "    return json_repair.loads(message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_h1(html_header_splits):\n",
    "    for html in html_header_splits:\n",
    "        if \"Header 1\" in html.metadata.keys():\n",
    "            return html.metadata[\"Header 1\"]\n",
    "            \n",
    "# get_h1(html_header_splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sub_header(split):\n",
    "    if 'Header 3' in split.metadata.keys():\n",
    "        return split.metadata['Header 3']\n",
    "    elif 'Header 2' in split.metadata.keys():\n",
    "        return split.metadata['Header 2']\n",
    "    elif 'Header 1' in split.metadata.keys():\n",
    "        return split.metadata['Header 1']\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "# get_sub_header(char_splits[12])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from langchain_text_splitters import HTMLHeaderTextSplitter\n",
    "\n",
    "def get_text_splits_from_text(texts):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=4000, \n",
    "        chunk_overlap=100,        \n",
    "    )\n",
    "\n",
    "    return text_splitter.split_documents(texts)\n",
    "\n",
    "def get_text_from_html(url = None, text = None):\n",
    "    headers_to_split_on = [\n",
    "        (\"h1\", \"Header 1\"),\n",
    "        (\"h2\", \"Header 2\"),\n",
    "        (\"h3\", \"Header 3\"),\n",
    "        (\"h4\", \"Header 4\"),\n",
    "        (\"h5\", \"Header 5\"),\n",
    "    ]\n",
    "\n",
    "    html_splitter = HTMLHeaderTextSplitter(headers_to_split_on=headers_to_split_on)\n",
    "\n",
    "    if url:\n",
    "        html_header_splits = html_splitter.split_text_from_url(url)\n",
    "    elif text:\n",
    "        html_header_splits = html_splitter.split_text(text)\n",
    "    \n",
    "    return html_header_splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_list = []\n",
    "question_list = []\n",
    "try_later_split_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_questions(splits, llm, parameters, html_header_splits):\n",
    "    print( \"------------------- generate_questions FUNCTION -------------------\" )\n",
    "    \n",
    "    i = 0\n",
    "    for split in splits:\n",
    "        print( \"-------------------- split --------------------\" )\n",
    "        print( split )\n",
    "        print( \"-------------------- split --------------------\" )\n",
    "        \n",
    "        try:\n",
    "            chain = prompt_question_generator | llm\n",
    "            \n",
    "            parameters[\"domain_knowledge\"] = split.page_content\n",
    "            \n",
    "            response = chain.invoke(parameters)\n",
    "            \n",
    "            response_list.append(response)\n",
    "            \n",
    "            questions = json_parser(response)\n",
    "            \n",
    "            h1 = get_h1(html_header_splits)\n",
    "            \n",
    "            sub_header = get_sub_header(split)\n",
    "            if sub_header is None:\n",
    "                sub_header = h1\n",
    "            \n",
    "            for q in questions:\n",
    "                q[\"subject_matter_1\"] = h1\n",
    "                q[\"subject_matter_2\"] = sub_header\n",
    "            \n",
    "            question_list.extend(questions)\n",
    "            \n",
    "            data, count = supabase.table('questions').insert(questions).execute()\n",
    "        except Exception as e:\n",
    "            print(\"An error occurred:\", e)\n",
    "            try_later_split_list.append(split)\n",
    "        \n",
    "        # i +=1\n",
    "        # if i == 2:\n",
    "        #     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urlparse, urljoin\n",
    "\n",
    "def extract_links_from_url(url):\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        if response.status_code == 200:\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            # base_url = urlparse(url).scheme + '://' + urlparse(url).netloc\n",
    "            base_url = url\n",
    "            links = []\n",
    "            for link in soup.find_all('a', href=True):\n",
    "                href = link.get('href')\n",
    "                if href.startswith('http'):\n",
    "                    links.append(href)\n",
    "                else:\n",
    "                    links.append(urljoin(base_url, href))\n",
    "            return links\n",
    "        else:\n",
    "            print(f\"Failed to retrieve page: {response.status_code}\")\n",
    "            return []\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_links(links):\n",
    "    i = len(links)-1\n",
    "    while i >= 0:\n",
    "        # print(links[i])\n",
    "        if links[i] == 'https://aman.ai':\n",
    "            links.pop(i)\n",
    "        elif links[i] == 'https://aman.ai/':\n",
    "            links.pop(i)\n",
    "        elif links[i] == 'https://aman.ai/primers/ai/ml-comp/':\n",
    "            links.pop(i)\n",
    "        elif links[i] == 'https://aman.ai/cs229/linear-regression/':\n",
    "            links.pop(i)\n",
    "        elif links[i].startswith(\"https://aman.ai\") == False:\n",
    "            links.pop(i)\n",
    "        i -= 1\n",
    "        \n",
    "    return links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# main_urls = [\n",
    "#     \"https://aman.ai/cs229/\",\n",
    "#     \"https://aman.ai/cs230/\",\n",
    "#     \"https://aman.ai/cs231n/\",\n",
    "#     \"https://aman.ai/cs224n/\",\n",
    "#     \"https://aman.ai/coursera-ml/\",\n",
    "#     \"https://aman.ai/recsys/index.html\",\n",
    "#     \"https://aman.ai/coursera-dl/\",\n",
    "#     \"https://aman.ai/coursera-nlp/\",\n",
    "#     \"https://aman.ai/multimodal/\",\n",
    "#     \"https://aman.ai/primers/ai/\",\n",
    "#     \"https://aman.ai/primers/graph/\",\n",
    "#     \"https://aman.ai/primers/numpy/\",\n",
    "#     \"https://aman.ai/primers/matplotlib/\",\n",
    "#     \"https://aman.ai/primers/pandas/\",\n",
    "#     \"https://aman.ai/primers/python/\",\n",
    "#     \"https://aman.ai/primers/tensorflow/\",\n",
    "#     \"https://aman.ai/primers/backprop/\",\n",
    "#     \"https://aman.ai/primers/math/\",\n",
    "#     \"https://aman.ai/code/\",\n",
    "#     \"https://aman.ai/code/data-structures/\",\n",
    "#     \"https://aman.ai/code/asymptotic-notations/\",\n",
    "# ]\n",
    "\n",
    "main_urls = [\n",
    "    # \"https://aman.ai/primers/math/\", ok\n",
    "    \"https://aman.ai/primers/ai/\",\n",
    "    \"https://aman.ai/primers/numpy/\",\n",
    "    \"https://aman.ai/primers/pandas/\",\n",
    "    \"https://aman.ai/primers/python/\",\n",
    "    \"https://aman.ai/primers/matplotlib/\",\n",
    "    \"https://aman.ai/primers/tensorflow/\",\n",
    "    \"https://aman.ai/code/\",\n",
    "    \"https://aman.ai/code/data-structures/\",\n",
    "    \"https://aman.ai/code/asymptotic-notations/\",\n",
    "    \"https://aman.ai/coursera-ml/\",\n",
    "    \"https://aman.ai/coursera-dl/\",\n",
    "    \"https://aman.ai/coursera-nlp/\",\n",
    "    \"https://aman.ai/cs229/\",\n",
    "    \"https://aman.ai/cs230/\",\n",
    "    \"https://aman.ai/cs231n/\",\n",
    "    \"https://aman.ai/cs224n/\",\n",
    "    \"https://aman.ai/recsys/index.html\",\n",
    "    \"https://aman.ai/multimodal/\",\n",
    "    \"https://aman.ai/primers/graph/\",\n",
    "    \"https://aman.ai/primers/backprop/\",\n",
    "]\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.disable(logging.DEBUG)\n",
    "\n",
    "# Re-enable debug logs\n",
    "# logging.disable(logging.NOTSET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print( \"Quantity of main_urls\", len( main_urls ) )\n",
    "\n",
    "for i_main_urls, url in enumerate(main_urls):\n",
    "    print(\"\")\n",
    "    print( \"main_urls #\", i_main_urls )\n",
    "    print( \"main url:\", url )\n",
    "    \n",
    "    internal_links = extract_links_from_url(url)\n",
    "    internal_links_cleaned = clean_links(internal_links)\n",
    "    \n",
    "    print(\"\")\n",
    "    print( \"Quantity of internal_links_cleaned\", len( internal_links_cleaned ) )\n",
    "    \n",
    "    for i_internal_links_cleaned, link in enumerate(internal_links_cleaned):\n",
    "        print(\"\")\n",
    "        print( \"internal link #\", i_internal_links_cleaned )\n",
    "        print( \"internal link\", link )\n",
    "        \n",
    "        html_header_splits = get_text_from_html(link)        \n",
    "        splits  = get_text_splits_from_text(html_header_splits)\n",
    "        \n",
    "        for level in [\"beginner\", \"intermediate\", \"hard\"]:\n",
    "            print( \"level:\", level )\n",
    "            \n",
    "            parameters = {\n",
    "                \"quantity\": 5,\n",
    "                \"level\": level,\n",
    "            }\n",
    "            \n",
    "            print(\"\")\n",
    "            print( \"Quantity of splits\", len( splits ) )\n",
    "            \n",
    "            generate_questions(splits, llm_google, parameters, html_header_splits)\n",
    "            \n",
    "            \n",
    "    #         break\n",
    "    #     break\n",
    "    # break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
