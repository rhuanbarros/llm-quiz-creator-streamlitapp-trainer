{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspaces/llm-quiz-creator-streamlitapp-trainer/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# vector database\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "# from langchain_community.vectorstores import Qdrant\n",
    "\n",
    "# ingestion\n",
    "from langchain_core.documents import Document\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter, CharacterTextSplitter\n",
    "\n",
    "# from llama_index.core import SimpleDirectoryReader\n",
    "\n",
    "# chat\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.messages import AIMessage, HumanMessage\n",
    "from langchain_core.messages.base import BaseMessage\n",
    "\n",
    "from langchain.prompts import FewShotPromptTemplate, PromptTemplate\n",
    "\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "\n",
    "from pydantic import BaseModel\n",
    "from typing import List, Optional\n",
    "from langchain.output_parsers import PydanticOutputParser\n",
    "import json\n",
    "import re\n",
    "\n",
    "# system\n",
    "import os\n",
    "import logging\n",
    "import sys\n",
    "\n",
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "logging.basicConfig(level=logging.DEBUG,  # Define o nível de log\n",
    "                    format='%(asctime)s - %(levelname)s - %(message)s',  # Define o formato da mensagem de log\n",
    "                    stream=sys.stdout)  # Define a saída do log para stdout\n",
    "                    # filename='app.log',  # Define o arquivo onde os logs serão gravados\n",
    "                    # filemode='a')  # Define o modo de escrita do arquivo de log (append)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()  # take environment variables from .env."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-07-29 11:42:58,936 - INFO - Inicializando LLM e embedings\n"
     ]
    }
   ],
   "source": [
    "logging.info('Inicializando LLM e embedings')\n",
    "llm_google = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash\", convert_system_message_to_human=True)\n",
    "# embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\", google_api_key=api_key_google)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain_openai import ChatOpenAI\n",
    "# from langchain_openai import ChatOpenAI\n",
    "\n",
    "# llm_openai = ChatOpenAI(openai_api_key=\"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\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "object_schema = \"\"\"\n",
    "{\n",
    "  \"properties\": {\n",
    "    \"topic_description\": {\n",
    "      \"type\": \"string\",\n",
    "      \"description\": \"A sentence describing the sub-topic to which the question belongs. That means this sentence should specify in a granular level what specific sub-topic the question belongs to. It should be abstract in a way that other questions could be put in this description too. Use between 5 and 10 words.\"\n",
    "    },\n",
    "    \"level\": {\n",
    "      \"type\": \"string\",\n",
    "      \"description\": \"The difficulty level of the question. It should be only one of the following options: 'beginner', 'intermediate', 'advanced'.\"\n",
    "    },\n",
    "    \"question\": {\n",
    "      \"type\": \"string\",\n",
    "      \"description\": \"The actual question text. It should be a question of type TRUE or FALSE. It means that the questions should be an assertion that could be answered with TRUE or FALSE.\"\n",
    "    },\n",
    "    \"answer\": {\n",
    "      \"type\": \"string\",\n",
    "      \"description\": \"The correct answer to the question. It should be only one of the following options: TRUE or FALSE\"\n",
    "    },\n",
    "    \"explanation\": {\n",
    "      \"type\": \"string\",\n",
    "      \"description\": \"An explanation or solution to the question.\"\n",
    "    }\n",
    "  },\n",
    "  \"required\": [\"topic_description\", \"level\", \"question\", \"answer\", \"explanation\"]\n",
    "}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_question_generator = PromptTemplate(\n",
    "    template=\"\"\"\n",
    "                TASK CONTEXT:\n",
    "                I am studying machine learning and I need to practice some questions on various topics.\n",
    "                \n",
    "                TASK DESCRIPTION:\n",
    "                I will provide you with a list of topics, and I would like you to generate a list of TRUE or FALSE questions.\n",
    "                These questions should be interesting, creative, challenging and thought-provoking. \n",
    "                Each question should be in the form of a statement that could be either TRUE or FALSE.\n",
    "                Feel free to be imaginative and attempt to confuse the student by blending related concepts or similar words.\n",
    "                I will provide the topics in the DOMAIN KNOWLEDGE section.\n",
    "                The questions should pertain to these topics, and you can use this knowledge as a foundation to create questions that delve deeper into the subject matter.\n",
    "                \n",
    "                ADDITIONAL TASK DESCRIPTION:\n",
    "                {additional_task_description}\n",
    "                \n",
    "                TASK REQUIREMENTS:\n",
    "                Please refrain from creating questions that require mathematical calculations, but you may create questions with mathematical formulas.\n",
    "                You SHOULD use LATEX to write mathematical formulas and code, but you should use the Katex flavor.\n",
    "                Also you should put $$ in the beggining of the katex code and $$ at the end of the code. This is necessary because the interpreter needs it.\n",
    "                \n",
    "                TASK DETAILS:\n",
    "                You should create {quantity} questions of level {level}.\n",
    "                \n",
    "                DOMAIN KNOWLEDGE:\n",
    "                {domain_knowledge}\n",
    "                \n",
    "                FORMAT OUTPUT INSTRUCTIONS:\n",
    "                The output should be formatted as a JSON list of objects that conforms class object schema below.\n",
    "                You should output just the Json list. \n",
    "                You should not output any other word like \"json\" in the beginning because it will ruin the parser.\n",
    "\n",
    "                ```\n",
    "                {object_schema}\n",
    "                ```\n",
    "            \"\"\",\n",
    "    input_variables=[\"quantity\", \"level\", \"additional_task_description\"],\n",
    "    partial_variables={\"object_schema\": object_schema},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from supabase import create_client, Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-07-29 11:42:59,172 - DEBUG - load_ssl_context verify=True cert=None trust_env=True http2=False\n",
      "2024-07-29 11:42:59,174 - DEBUG - load_verify_locations cafile='/workspaces/llm-quiz-creator-streamlitapp-trainer/.venv/lib/python3.10/site-packages/certifi/cacert.pem'\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "url: str = os.environ.get(\"SUPABASE_URL\")\n",
    "key: str = os.environ.get(\"SUPABASE_KEY\")\n",
    "supabase: Client = create_client(url, key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import  json_repair\n",
    "\n",
    "def json_parser(message: AIMessage) -> List[dict]:\n",
    "    return json_repair.loads(message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_questions(text, llm, parameters, subject_matter_1, subject_matter_2):\n",
    "    print( \"------------------- generate_questions FUNCTION -------------------\" )\n",
    "    \n",
    "    if text is None:\n",
    "        raise Exception(\"text is None\")\n",
    "        \n",
    "    try:\n",
    "        chain = prompt_question_generator | llm\n",
    "        \n",
    "        parameters[\"domain_knowledge\"] = text\n",
    "        \n",
    "        response = chain.invoke(parameters)\n",
    "        \n",
    "        questions = json_parser(response)\n",
    "        \n",
    "        for q in questions:\n",
    "            q[\"subject_matter_1\"] = subject_matter_1\n",
    "            q[\"subject_matter_2\"] = subject_matter_2    \n",
    "        \n",
    "        data, count = supabase.table('questions').insert(questions).execute()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(\"An error occurred:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.disable(logging.DEBUG)\n",
    "\n",
    "# Re-enable debug logs\n",
    "# logging.disable(logging.NOTSET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"\n",
    "Machine Learning Metrics\n",
    "1. Classification Metrics\n",
    "Accuracy:\n",
    "Definition and calculation\n",
    "Pros and cons: sensitivity to class imbalance\n",
    "Use cases: overall performance evaluation\n",
    "Precision:\n",
    "Definition and calculation\n",
    "Meaning: ability to correctly identify positive cases\n",
    "Use cases: evaluating models with high false positive costs\n",
    "Recall:\n",
    "Definition and calculation\n",
    "Meaning: ability to correctly identify all positive cases\n",
    "Use cases: evaluating models with high false negative costs\n",
    "F1-Score:\n",
    "Definition and calculation as harmonic mean of precision and recall\n",
    "Use cases: balancing precision and recall\n",
    "Specificity:\n",
    "Definition and calculation\n",
    "Meaning: ability to correctly identify negative cases\n",
    "Use cases: evaluating models with high false negative costs\n",
    "ROC Curve and AUC:\n",
    "Definition and interpretation of ROC curve\n",
    "Calculation of Area Under the Curve (AUC)\n",
    "Use cases: comparing models with different thresholds\n",
    "Confusion Matrix:\n",
    "Definition and interpretation of matrix elements\n",
    "Visualization of classification performance\n",
    "Use cases: detailed analysis of model errors\n",
    "2. Regression Metrics\n",
    "Mean Absolute Error (MAE):\n",
    "Definition and calculation\n",
    "Meaning: average absolute difference between predicted and actual values\n",
    "Use cases: sensitive to outliers\n",
    "Mean Squared Error (MSE):\n",
    "Definition and calculation\n",
    "Meaning: average squared difference between predicted and actual values\n",
    "Use cases: penalizes large errors more heavily\n",
    "Root Mean Squared Error (RMSE):\n",
    "Definition and calculation as square root of MSE\n",
    "Meaning: provides error in the same units as the target variable\n",
    "Use cases: easy to interpret and compare to other models\n",
    "R-squared:\n",
    "Definition and calculation as proportion of variance explained\n",
    "Meaning: indicates how well the model fits the data\n",
    "Use cases: evaluating model fit for continuous data\n",
    "Adjusted R-squared:\n",
    "Definition and calculation, accounting for number of predictors\n",
    "Meaning: adjusts R-squared for model complexity\n",
    "Use cases: comparing models with different numbers of features\n",
    "3. Model Evaluation and Selection\n",
    "Cross-validation:\n",
    "K-fold cross-validation\n",
    "Leave-one-out cross-validation\n",
    "Use cases: evaluating model performance on unseen data\n",
    "Hyperparameter Tuning:\n",
    "Grid search\n",
    "Random search\n",
    "Use cases: optimizing model performance by adjusting hyperparameters\n",
    "Model Comparison:\n",
    "Statistical significance tests: t-test, F-test\n",
    "Comparing models based on chosen metrics\n",
    "Use cases: choosing the best model for a specific task\n",
    "Bias-Variance Trade-off:\n",
    "Understanding the trade-off between model bias and variance\n",
    "Impact on model performance\n",
    "Use cases: choosing a model that balances complexity and accuracy\n",
    "4. Advanced Metrics\n",
    "Log Loss:\n",
    "Definition and calculation for probabilistic models\n",
    "Meaning: measures model confidence in its predictions\n",
    "Use cases: evaluating models with multiple classes\n",
    "KL Divergence:\n",
    "Definition and calculation for comparing probability distributions\n",
    "Meaning: measures dissimilarity between two distributions\n",
    "Use cases: evaluating model calibration\n",
    "Jaccard Index:\n",
    "Definition and calculation for comparing sets\n",
    "Meaning: measures overlap between predicted and actual sets\n",
    "Use cases: evaluating models for object detection and segmentation\n",
    "Ranking Metrics:\n",
    "Mean Reciprocal Rank (MRR)\n",
    "Normalized Discounted Cumulative Gain (NDCG)\n",
    "Use cases: evaluating models for ranking and recommendation tasks\n",
    "5. Application-Specific Metrics\n",
    "Sentiment Analysis:\n",
    "Accuracy, precision, recall for sentiment classification\n",
    "Sentiment polarity scores for fine-grained analysis\n",
    "Natural Language Processing:\n",
    "BLEU, ROUGE for machine translation and text summarization\n",
    "Perplexity for language models\n",
    "Computer Vision:\n",
    "Intersection over Union (IoU) for object detection and segmentation\n",
    "Mean Average Precision (mAP) for multi-object detection\n",
    "Recommender Systems:\n",
    "Precision, recall, F1-score for recommendation accuracy\n",
    "Rank correlation metrics for evaluating ranking quality\n",
    "Time Series Forecasting:\n",
    "Root Mean Squared Error (RMSE), Mean Absolute Error (MAE)\n",
    "Mean Absolute Percentage Error (MAPE) for relative error evaluation\n",
    "Fraud Detection:\n",
    "Area Under the ROC Curve (AUC) for imbalanced datasets\n",
    "False Positive Rate (FPR) and False Negative Rate (FNR) for cost-sensitive evaluation\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "level: beginner\n",
      "------------------- generate_questions FUNCTION -------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1722253379.278750    6767 config.cc:230] gRPC experiments enabled: call_status_override_on_cancellation, event_engine_dns, event_engine_listener, http2_stats_fix, monitoring_experiment, pick_first_new, trace_record_callops, work_serializer_clears_time_cache\n",
      "I0000 00:00:1722253379.338672    6767 check_gcp_environment.cc:61] BIOS data file does not exist or cannot be opened.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-07-29 11:43:10,157 - INFO - HTTP Request: POST https://xoxlgvakygiyfijfeixu.supabase.co/rest/v1/questions?columns=%22explanation%22%2C%22question%22%2C%22level%22%2C%22answer%22%2C%22topic_description%22%2C%22subject_matter_2%22%2C%22subject_matter_1%22 \"HTTP/2 201 Created\"\n",
      "level: intermediate\n",
      "------------------- generate_questions FUNCTION -------------------\n",
      "2024-07-29 11:43:19,463 - INFO - HTTP Request: POST https://xoxlgvakygiyfijfeixu.supabase.co/rest/v1/questions?columns=%22explanation%22%2C%22question%22%2C%22level%22%2C%22answer%22%2C%22topic_description%22%2C%22subject_matter_2%22%2C%22subject_matter_1%22 \"HTTP/2 201 Created\"\n",
      "level: hard\n",
      "------------------- generate_questions FUNCTION -------------------\n",
      "An error occurred: index: 0\n",
      "finish_reason: RECITATION\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for level in [\n",
    "    \"beginner\", \n",
    "    \"intermediate\", \n",
    "    \"hard\"\n",
    "    ]:\n",
    "    print( \"level:\", level )\n",
    "    \n",
    "    parameters = {\n",
    "        \"quantity\": 10,\n",
    "        \"level\": level,\n",
    "        \"additional_task_description\": \"Create questions only about the definitions of the concepts, like mixing the definition of one with another distribution, or mixing the use of one with the use of another. I need this to memorize this concepts.\"\n",
    "    }\n",
    "    \n",
    "    subject_matter_1 = \"Machine Learning\"\n",
    "    subject_matter_2 = \"Machine Learning  - Metrics\"\n",
    "    \n",
    "    generate_questions(text, llm_google, parameters, subject_matter_1, subject_matter_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
