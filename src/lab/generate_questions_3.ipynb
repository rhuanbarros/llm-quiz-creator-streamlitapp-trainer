{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rhuan/PROJETOS/quiz_proj2/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# vector database\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "from langchain_community.vectorstores import Qdrant\n",
    "\n",
    "# ingestion\n",
    "from langchain_core.documents import Document\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter, CharacterTextSplitter\n",
    "\n",
    "# from llama_index.core import SimpleDirectoryReader\n",
    "\n",
    "# chat\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.messages import AIMessage, HumanMessage\n",
    "from langchain_core.messages.base import BaseMessage\n",
    "\n",
    "from langchain.prompts import FewShotPromptTemplate, PromptTemplate\n",
    "\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "\n",
    "from pydantic import BaseModel\n",
    "from typing import List, Optional\n",
    "from langchain.output_parsers import PydanticOutputParser\n",
    "import json\n",
    "import re\n",
    "\n",
    "# system\n",
    "import os\n",
    "import logging\n",
    "import sys\n",
    "\n",
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "logging.basicConfig(level=logging.DEBUG,  # Define o nível de log\n",
    "                    format='%(asctime)s - %(levelname)s - %(message)s',  # Define o formato da mensagem de log\n",
    "                    stream=sys.stdout)  # Define a saída do log para stdout\n",
    "                    # filename='app.log',  # Define o arquivo onde os logs serão gravados\n",
    "                    # filemode='a')  # Define o modo de escrita do arquivo de log (append)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = \"quiz\"\n",
    "os.environ[\"LANGCHAIN_ENDPOINT\"] = \"https://api.smith.langchain.com\"\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = \"ls__b626f8e0970e43cca449e7a3510ac96b\"  # Update to your API key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-04-01 08:38:07,183 - INFO - Inicializando LLM e embedings\n"
     ]
    }
   ],
   "source": [
    "logging.info('Inicializando LLM e embedings')\n",
    "api_key_google = \"AIzaSyC-V6lfROehy46ntB6zPZ7CJ8zNF3gDdO4\"\n",
    "llm_google = ChatGoogleGenerativeAI(model=\"gemini-pro\", convert_system_message_to_human=True, google_api_key=api_key_google)\n",
    "embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\", google_api_key=api_key_google)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-04-01 08:38:08,299 - DEBUG - load_ssl_context verify=True cert=None trust_env=True http2=False\n",
      "2024-04-01 08:38:08,301 - DEBUG - load_verify_locations cafile='/home/rhuan/PROJETOS/quiz_proj2/.venv/lib/python3.10/site-packages/certifi/cacert.pem'\n",
      "2024-04-01 08:38:08,338 - DEBUG - load_ssl_context verify=True cert=None trust_env=True http2=False\n",
      "2024-04-01 08:38:08,339 - DEBUG - load_verify_locations cafile='/home/rhuan/PROJETOS/quiz_proj2/.venv/lib/python3.10/site-packages/certifi/cacert.pem'\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm_openai = ChatOpenAI(openai_api_key=\"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\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "object_schema = \"\"\"\n",
    "{\n",
    "  \"properties\": {\n",
    "    \"topic_description\": {\n",
    "      \"type\": \"string\",\n",
    "      \"description\": \"A sentence describing the sub-topic to which the question belongs. That means this sentence should specify in a granular level what specific sub-topic the question belongs to. It should be abstract in a way that other questions could be put in this description too. Use between 5 and 10 words.\"\n",
    "    },\n",
    "    \"level\": {\n",
    "      \"type\": \"string\",\n",
    "      \"description\": \"The difficulty level of the question. It should be only one of the following options: 'beginner', 'intermediate', 'advanced'.\"\n",
    "    },\n",
    "    \"question\": {\n",
    "      \"type\": \"string\",\n",
    "      \"description\": \"The actual question text. It should be a question of type TRUE or FALSE. It means that the questions should be an assertion that could be answered with TRUE or FALSE.\"\n",
    "    },\n",
    "    \"answer\": {\n",
    "      \"type\": \"string\",\n",
    "      \"description\": \"The correct answer to the question. It should be only one of the following options: TRUE or FALSE\"\n",
    "    },\n",
    "    \"explanation\": {\n",
    "      \"type\": \"string\",\n",
    "      \"description\": \"An explanation or solution to the question.\"\n",
    "    }\n",
    "  },\n",
    "  \"required\": [\"topic_description\", \"level\", \"question\", \"answer\", \"explanation\"]\n",
    "}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_question_generator = PromptTemplate(\n",
    "    template=\"\"\"\n",
    "                TASK CONTEXT:\n",
    "                I am studying machine learning and I need to practice some questions on various topics.\n",
    "                \n",
    "                TASK DESCRIPTION:\n",
    "                I will provide you with a list of topics, and I would like you to generate a list of TRUE or FALSE questions.\n",
    "                These questions should be interesting, creative, challenging and thought-provoking. \n",
    "                Each question should be in the form of a statement that could be either TRUE or FALSE.\n",
    "                Feel free to be imaginative and attempt to confuse the student by blending related concepts or similar words.\n",
    "                I will provide the topics in the DOMAIN KNOWLEDGE section.\n",
    "                The questions should pertain to these topics, and you can use this knowledge as a foundation to create questions that delve deeper into the subject matter.\n",
    "                \n",
    "                ADDITIONAL TASK DESCRIPTION:\n",
    "                {additional_task_description}\n",
    "                \n",
    "                TASK REQUIREMENTS:\n",
    "                Please refrain from creating questions that require mathematical calculations, but you may create questions with mathematical formulas.\n",
    "                You SHOULD use LATEX to write mathematical formulas and code, but you should use the Katex flavor.\n",
    "                Also you should put $$ in the beggining of the katex code and $$ at the end of the code. This is necessary because the interpreter needs it.\n",
    "                \n",
    "                TASK DETAILS:\n",
    "                You should create {quantity} questions of level {level}.\n",
    "                \n",
    "                DOMAIN KNOWLEDGE:\n",
    "                {domain_knowledge}\n",
    "                \n",
    "                FORMAT OUTPUT INSTRUCTIONS:\n",
    "                The output should be formatted as a JSON list of objects that conforms class object schema below.\n",
    "                You should output just the Json list. \n",
    "                You should not output any other word like \"json\" in the beginning because it will ruin the parser.\n",
    "\n",
    "                ```\n",
    "                {object_schema}\n",
    "                ```\n",
    "            \"\"\",\n",
    "    input_variables=[\"quantity\", \"level\", \"additional_task_description\"],\n",
    "    partial_variables={\"object_schema\": object_schema},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from supabase import create_client, Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-04-01 08:38:11,556 - DEBUG - load_ssl_context verify=True cert=None trust_env=True http2=False\n",
      "2024-04-01 08:38:11,557 - DEBUG - load_verify_locations cafile='/home/rhuan/PROJETOS/quiz_proj2/.venv/lib/python3.10/site-packages/certifi/cacert.pem'\n"
     ]
    }
   ],
   "source": [
    "url = \"https://xoxlgvakygiyfijfeixu.supabase.co\"\n",
    "key = \"eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJzdXBhYmFzZSIsInJlZiI6InhveGxndmFreWdpeWZpamZlaXh1Iiwicm9sZSI6InNlcnZpY2Vfcm9sZSIsImlhdCI6MTcwNDkyNzU2NywiZXhwIjoyMDIwNTAzNTY3fQ.V3766GRj6hkt1Ci-52tjSiULVoF3nfCPPDnR6Hc_rT0\"\n",
    "\n",
    "supabase: Client = create_client(url, key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import  json_repair\n",
    "\n",
    "def json_parser(message: AIMessage) -> List[dict]:\n",
    "    return json_repair.loads(message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_questions(text, llm, parameters, subject_matter_1, subject_matter_2):\n",
    "    print( \"------------------- generate_questions FUNCTION -------------------\" )\n",
    "    \n",
    "    if text is None:\n",
    "        raise Exception(\"text is None\")\n",
    "        \n",
    "    try:\n",
    "        chain = prompt_question_generator | llm\n",
    "        \n",
    "        parameters[\"domain_knowledge\"] = text\n",
    "        \n",
    "        response = chain.invoke(parameters)\n",
    "        \n",
    "        questions = json_parser(response)\n",
    "        \n",
    "        for q in questions:\n",
    "            q[\"subject_matter_1\"] = subject_matter_1\n",
    "            q[\"subject_matter_2\"] = subject_matter_2    \n",
    "        \n",
    "        data, count = supabase.table('questions').insert(questions).execute()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(\"An error occurred:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.disable(logging.DEBUG)\n",
    "\n",
    "# Re-enable debug logs\n",
    "# logging.disable(logging.NOTSET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"\n",
    "Normal Distribution:\n",
    "\n",
    "Main Use: Widely used in statistical analysis and modeling due to its symmetrical bell shape. It describes many natural phenomena and is central to the theory of probability and statistics.\n",
    "Binomial Distribution:\n",
    "\n",
    "Main Use: Applicable when there are two possible outcomes, typically success or failure, in a series of independent trials. Used in scenarios like coin flipping, pass/fail rates, and binary classification problems.\n",
    "Poisson Distribution:\n",
    "\n",
    "Main Use: Suitable for modeling the number of events occurring in a fixed interval of time or space when these events occur with a known constant mean rate and are independent of the time since the last event. Commonly applied in areas such as queuing theory, insurance, and telecommunications.\n",
    "Exponential Distribution:\n",
    "\n",
    "Main Use: Describes the time between events in a Poisson process, where events occur continuously and independently at a constant average rate. Frequently used in reliability engineering to model time until failure and in queuing theory for inter-arrival times.\n",
    "Uniform Distribution:\n",
    "\n",
    "Main Use: All outcomes are equally likely. Often used for random number generation and in scenarios where every outcome within a range is equally probable, such as in Monte Carlo simulations and random sampling.\n",
    "Gamma Distribution:\n",
    "\n",
    "Main Use: Generalizes the exponential distribution to allow for a longer tail, making it suitable for modeling waiting times until a certain number of events have occurred. Widely used in fields such as physics, engineering, and finance for modeling time until failure and in queuing systems.\n",
    "Beta Distribution:\n",
    "\n",
    "Main Use: Flexible distribution used to model random variables that represent proportions or probabilities. Commonly used in Bayesian statistics, modeling proportions, and in quality control.\n",
    "Geometric Distribution:\n",
    "\n",
    "Main Use: Describes the number of Bernoulli trials needed to get one success. Useful in situations where you are interested in the number of trials until the first success, such as in modeling the number of attempts needed before a click on an online ad.\n",
    "Hypergeometric Distribution:\n",
    "\n",
    "Main Use: Describes the probability of k successes in n draws from a finite population without replacement. Often used in sampling without replacement problems, such as quality control inspections or opinion polls.\n",
    "Multinomial Distribution:\n",
    "\n",
    "Main Use: Generalization of the binomial distribution to more than two categories. Used in scenarios where there are more than two possible outcomes in each trial, such as in genetics, market research, and ecology.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "level: beginner\n",
      "------------------- generate_questions FUNCTION -------------------\n",
      "2024-04-01 08:50:27,840 - INFO - HTTP Request: POST https://xoxlgvakygiyfijfeixu.supabase.co/rest/v1/questions \"HTTP/1.1 201 Created\"\n"
     ]
    }
   ],
   "source": [
    "for level in [\n",
    "    \"beginner\", \n",
    "    # \"intermediate\", \n",
    "    # \"hard\"\n",
    "    ]:\n",
    "    print( \"level:\", level )\n",
    "    \n",
    "    parameters = {\n",
    "        \"quantity\": 10,\n",
    "        \"level\": level,\n",
    "        \"additional_task_description\": \"Create questions only about the definitions of the concepts, like mixing the definition of one with another distribution, or mixing the use of one with the use of another. I need this to memorize this concepts.\"\n",
    "    }\n",
    "    \n",
    "    subject_matter_1 = \"Math - Probability distributions - Concepts\"\n",
    "    subject_matter_2 = \"Math - Probability distributions - Concepts\"\n",
    "    \n",
    "    generate_questions(text, llm_google, parameters, subject_matter_1, subject_matter_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
