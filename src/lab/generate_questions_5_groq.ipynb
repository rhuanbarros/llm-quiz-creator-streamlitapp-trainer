{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()  # take environment variables from .env."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "\n",
    "class Question(BaseModel):\n",
    "    \"\"\" A question about a domain of study.\"\"\"\n",
    "\n",
    "    topic_description: str = Field(\n",
    "        description=\"A sentence describing the sub-topic to which the question belongs. That means this sentence should specify in a granular level what specific sub-topic the question belongs to. It should be abstract in a way that other questions could be put in this description too. Use between 5 and 10 words.\"\n",
    "    )\n",
    "    level: str = Field(\n",
    "        description=\"The difficulty level of the question. It should be only one of the following options: 'easy', 'medium', 'hard'.\"\n",
    "    )\n",
    "    question: str = Field(\n",
    "        description=\"The actual question text. It should be a question of type TRUE or FALSE. It means that the questions should be an assertion that could be answered with TRUE or FALSE.\"\n",
    "    )\n",
    "    answer_correct: str = Field(\n",
    "        description=\"The correct answer to the question. It should be only one of the following options: TRUE or FALSE\"\n",
    "    )\n",
    "    explanation: str = Field(\n",
    "        description=\"An explanation or solution to the question.\"\n",
    "    )\n",
    "\n",
    "class QuestionList(BaseModel):\n",
    "    \"\"\"A list of Question class.\"\"\"\n",
    "\n",
    "    items: list[Question] = Field(\n",
    "        description=\"list of Question\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "\n",
    "llm_groq = ChatGroq(model=\"llama-3.1-70b-versatile\")\n",
    "llm_QuestionList = llm_groq.with_structured_output(QuestionList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_question_generator = PromptTemplate.from_template(\"\"\"\n",
    "\n",
    "    TASK CONTEXT:\n",
    "    I am studying machine learning and I need to practice some questions on various topics.\n",
    "    \n",
    "    TASK DESCRIPTION:\n",
    "    I will provide you with a list of topics, and I would like you to generate a list of TRUE or FALSE questions.\n",
    "    These questions should be interesting, creative, challenging and thought-provoking. \n",
    "    Each question should be in the form of a statement that could be either TRUE or FALSE.\n",
    "    Feel free to be imaginative and attempt to confuse the student by blending related concepts or similar words.\n",
    "    I will provide the topics in the DOMAIN KNOWLEDGE section.\n",
    "    The questions should pertain to these topics, and you can use this knowledge as a foundation to create questions that delve deeper into the subject matter.\n",
    "    \n",
    "    ADDITIONAL TASK DESCRIPTION:\n",
    "    {additional_task_description}\n",
    "    \n",
    "    TASK REQUIREMENTS:\n",
    "    Please refrain from creating questions that require mathematical calculations, but you may create questions with mathematical formulas.\n",
    "    You SHOULD use LATEX to write mathematical formulas and code, but you should use the Katex flavor.\n",
    "    Also you should put $$ in the beggining of the katex code and $$ at the end of the code. This is necessary because the interpreter needs it.\n",
    "    \n",
    "    TASK DETAILS:\n",
    "    You should create {quantity} questions of level {level}.\n",
    "    \n",
    "    DOMAIN KNOWLEDGE:\n",
    "    {domain_knowledge}\n",
    "    \n",
    "    FORMAT OUTPUT INSTRUCTIONS:\n",
    "    It should be formatted as described in the output format.\n",
    "    \"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from supabase import create_client, Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "url: str = os.environ.get(\"SUPABASE_URL\")\n",
    "key: str = os.environ.get(\"SUPABASE_KEY\")\n",
    "supabase: Client = create_client(url, key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"\n",
    "\n",
    "Language Models and Probability\n",
    "Entropy\n",
    "Cross-Entropy\n",
    "Cross-Entropy Loss\n",
    "Perplexity\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_questions(text, llm, parameters, subject_matter_1, subject_matter_2):\n",
    "    print( \"------------------- generate_questions FUNCTION -------------------\" )\n",
    "    \n",
    "    if text is None:\n",
    "        raise Exception(\"text is None\")\n",
    "        \n",
    "    try:\n",
    "        chain = prompt_question_generator | llm\n",
    "        \n",
    "        parameters[\"domain_knowledge\"] = text\n",
    "        \n",
    "        questions = chain.invoke(parameters)\n",
    "\n",
    "        print(questions)\n",
    "               \n",
    "        # for q in questions:\n",
    "        #     q[\"subject_matter_1\"] = subject_matter_1\n",
    "        #     q[\"subject_matter_2\"] = subject_matter_2    \n",
    "        \n",
    "        # data, count = supabase.table('questions').insert(questions).execute()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(\"An error occurred:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "level: hard\n",
      "------------------- generate_questions FUNCTION -------------------\n",
      "items=[Question(topic_description='Language Models and Probability', level='hard', question='A language model that assigns a higher probability to a sentence is more likely to generate that sentence.', answer_correct='TRUE', explanation='A language model assigns probabilities to sentences based on their likelihood of occurrence. A higher probability indicates that the model is more confident in generating that sentence.'), Question(topic_description='Entropy', level='hard', question='Entropy is a measure of the amount of information in a probability distribution.', answer_correct='TRUE', explanation='Entropy is a measure of the uncertainty or randomness in a probability distribution. It quantifies the amount of information in the distribution.'), Question(topic_description='Cross-Entropy', level='hard', question='Cross-entropy is a measure of the difference between two probability distributions.', answer_correct='TRUE', explanation='Cross-entropy measures the difference between two probability distributions. It is often used in machine learning to measure the difference between the predicted probabilities and the true labels.'), Question(topic_description='Cross-Entropy Loss', level='hard', question='Cross-entropy loss is a loss function that measures the difference between the predicted probabilities and the true labels.', answer_correct='TRUE', explanation='Cross-entropy loss is a loss function that measures the difference between the predicted probabilities and the true labels. It is commonly used in classification problems.'), Question(topic_description='Perplexity', level='hard', question='Perplexity is a measure of how well a language model predicts a test set.', answer_correct='TRUE', explanation='Perplexity measures how well a language model predicts a test set. It is calculated as the exponential of the average cross-entropy loss.')]\n"
     ]
    }
   ],
   "source": [
    "for level in [\n",
    "    # \"beginner\", \n",
    "    # \"intermediate\", \n",
    "    \"hard\"\n",
    "    ]:\n",
    "    print( \"level:\", level )\n",
    "    \n",
    "    parameters = {\n",
    "        \"quantity\": 1,\n",
    "        \"level\": level,\n",
    "        \"additional_task_description\": \"Create questions only about the definitions of the concepts, like mixing the definition of one with another distribution, or mixing the use of one with the use of another. I need this to memorize this concepts.\"\n",
    "    }\n",
    "    \n",
    "    subject_matter_1 = \"Probability - LLMs\"\n",
    "    subject_matter_2 = subject_matter_1\n",
    "    \n",
    "    generate_questions(text, llm_QuestionList, parameters, subject_matter_1, subject_matter_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
